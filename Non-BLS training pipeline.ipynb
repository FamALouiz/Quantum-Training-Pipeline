{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7386b79d-4ad1-4bbc-8698-f4ef555ac255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA SUMMARY ===\n",
      "df shape: (24400, 55)\n",
      "time range: 2021-01-10 00:00:00 → 2021-02-10 19:00:00\n",
      "#tiles: 400\n",
      "#features (F): 50\n",
      "first 10 feature cols: ['DUEXTTAU', 'BCFLUXU', 'OCFLUXV', 'BCANGSTR', 'SUFLUXV', 'SSSMASS25', 'SSSMASS', 'OCSMASS', 'BCCMASS', 'BCSMASS']\n",
      "window config: L=8, H=4, stride=1\n",
      "#windows: 20000\n",
      "\n",
      "rows per tile (smallest 5):\n",
      "tile_id\n",
      "26.5_36.875    24\n",
      "28.5_30.625    24\n",
      "28.5_30.0      24\n",
      "28.5_29.375    24\n",
      "28.5_28.75     24\n",
      "Name: timestamp, dtype: int64\n",
      "rows per tile (largest 5):\n",
      "tile_id\n",
      "31.0_25.0    764\n",
      "26.0_25.0    764\n",
      "27.5_25.0    764\n",
      "25.5_25.0    764\n",
      "22.0_25.0    764\n",
      "Name: timestamp, dtype: int64\n",
      "\n",
      "=== FIRST WINDOW DEBUG ===\n",
      "tile_id: 22.0_25.0\n",
      "history shape [L,F]: (8, 50) | future shape [H]: (4,)\n",
      "hist timestamps: 2021-01-10 00:00:00 → 2021-01-10 07:00:00\n",
      "fut  timestamps: 2021-01-10 08:00:00 → 2021-01-10 11:00:00\n",
      "hist Δt seconds (unique): [3600.]\n",
      "fut  Δt seconds (unique): [3600.]\n",
      "x_hist window mean/std (overall): -0.3407861292362213 0.7213065028190613\n",
      "x_hist[0:5, 0] sample: [0.03592585772275925, 0.04441426694393158, 0.05713042989373207, 0.0698879063129425, 0.09415498375892639]\n",
      "y_future sample: [26.396438598632812, 25.872926712036133, 26.150850296020508, 27.159780502319336]\n",
      "\n",
      "=== BATCH CHECK (custom collate) ===\n",
      "batch x_hist shape [B,L,F]: (32, 8, 50)\n",
      "batch y_future shape [B,H]: (32, 4)\n",
      "meta keys: ['tile_id', 'start_time', 'lat', 'lon']\n",
      "tile_id[0]: 27.5_25.0\n",
      "start_time[0]: 2021-01-17 07:00:00\n",
      "lat/lon tensors: (32,) (32,)\n",
      "\n",
      "=== PATCHIFY SMOKE TEST ===\n",
      "P=4, S=2 -> N=3\n",
      "tokens shape [B,N,P*F]: (32, 3, 200)\n",
      "\n",
      "All sanity checks passed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def collate_keep_meta(batch):\n",
    "    # tensors\n",
    "    x = torch.stack([b[\"x_hist\"] for b in batch])      # [B, L, F]\n",
    "    y = torch.stack([b[\"y_future\"] for b in batch])    # [B, H]\n",
    "    lat = torch.tensor([b[\"lat\"] for b in batch], dtype=torch.float32)\n",
    "    lon = torch.tensor([b[\"lon\"] for b in batch], dtype=torch.float32)\n",
    "    # keep metadata as simple Python lists/strings\n",
    "    meta = {\n",
    "        \"tile_id\":   [b[\"tile_id\"] for b in batch],\n",
    "        \"start_time\": [str(b[\"start_time\"]) for b in batch],  # stringify Timestamps\n",
    "        \"lat\": lat, \"lon\": lon,\n",
    "    }\n",
    "    return {\"x_hist\": x, \"y_future\": y, \"meta\": meta}\n",
    "\n",
    "# ---------- 1) Identify columns ----------\n",
    "NON_FEATURE_COLS = {\n",
    "    \"lon\",\"lat\",\"time\",\"source_file\",\"PM25_MERRA2\",\"PM25_ug_m3\",\"class\"\n",
    "}\n",
    "\n",
    "def get_feature_cols(df: pd.DataFrame):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    feat_cols = [c for c in num_cols if c not in NON_FEATURE_COLS and c.lower() not in {\"timestamp\"}]\n",
    "    return feat_cols\n",
    "\n",
    "# ---------- 2) Parse & tidy ----------\n",
    "def prepare_dataframe(df: pd.DataFrame, hourly=True, dayfirst=True, freq=\"H\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Parses timestamps from df['time'].\n",
    "    - Builds tile_id from (lat, lon).\n",
    "    - Optionally resamples per tile to an evenly spaced time grid (freq='H' or '30T').\n",
    "    - Returns a tidy numeric frame where features are numeric and PM25_ug_m3 is present.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) timestamp + tile id\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"time\"], dayfirst=dayfirst, errors=\"coerce\")\n",
    "    df[\"tile_id\"] = (df[\"lat\"].round(4).astype(str) + \"_\" + df[\"lon\"].round(4).astype(str))\n",
    "\n",
    "    keep = [\"timestamp\", \"tile_id\", \"lat\", \"lon\", \"PM25_ug_m3\"] + get_feature_cols(df)\n",
    "    df = df[keep].dropna(subset=[\"timestamp\"]).sort_values([\"tile_id\", \"timestamp\"])\n",
    "\n",
    "    if hourly:\n",
    "        def _resample(g):\n",
    "            tile = g[\"tile_id\"].iloc[0]\n",
    "            lat0 = float(g[\"lat\"].iloc[0])\n",
    "            lon0 = float(g[\"lon\"].iloc[0])\n",
    "\n",
    "            g = g.set_index(\"timestamp\").sort_index()\n",
    "\n",
    "            # numeric-only columns for resampling (avoid strings like tile_id)\n",
    "            num_cols = g.select_dtypes(include=[np.number]).columns\n",
    "            g_num = g[num_cols].resample(freq).mean()  # numeric_only implicitly True on numeric subset\n",
    "\n",
    "            # fill gaps\n",
    "            g_num = g_num.interpolate(\"time\").ffill().bfill()\n",
    "\n",
    "            # add metadata back\n",
    "            g_num[\"tile_id\"] = tile\n",
    "            g_num[\"lat\"] = lat0\n",
    "            g_num[\"lon\"] = lon0\n",
    "\n",
    "            return g_num.reset_index()\n",
    "\n",
    "        df = df.groupby(\"tile_id\", group_keys=False).apply(_resample)\n",
    "\n",
    "    # ensure numeric dtypes and fill any leftovers\n",
    "    feat_cols = get_feature_cols(df)\n",
    "    df[feat_cols + [\"PM25_ug_m3\"]] = df[feat_cols + [\"PM25_ug_m3\"]].astype(float).fillna(0.0)\n",
    "    return df\n",
    "\n",
    "# ---------- 3) Compute normalization stats ----------\n",
    "def compute_feature_stats(df: pd.DataFrame):\n",
    "    feat_cols = get_feature_cols(df)\n",
    "    mean = df[feat_cols].mean().astype(\"float32\").values\n",
    "    std  = df[feat_cols].std(ddof=0).replace(0, 1.0).astype(\"float32\").values\n",
    "    return feat_cols, mean, std\n",
    "\n",
    "# ---------- 4) Window index builder ----------\n",
    "def build_indices(df: pd.DataFrame, L=168, H=72, stride=1):\n",
    "    idx = []\n",
    "    for tile, g in df.groupby(\"tile_id\"):\n",
    "        n = len(g)\n",
    "        for t in range(L, n - H + 1, stride):  # note +1\n",
    "            idx.append((tile, t))\n",
    "    return idx\n",
    "\n",
    "# ---------- 5) PyTorch Dataset ----------\n",
    "class TSWindowDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, L=168, H=72, stride=1, stats=None):\n",
    "        \"\"\"\n",
    "        df: output of prepare_dataframe()\n",
    "        L: lookback length (hours)\n",
    "        H: horizon length (72)\n",
    "        stats: (feat_cols, mean, std) from compute_feature_stats(train_df)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.L, self.H = L, H\n",
    "        self.feat_cols, self.mean, self.std = stats if stats is not None else compute_feature_stats(df)\n",
    "        self.idx = build_indices(df, L, H, stride)\n",
    "\n",
    "        # pre-slice groups to avoid repeated groupby in __getitem__\n",
    "        self.groups = {tile: g.reset_index(drop=True) for tile, g in df.groupby(\"tile_id\")}\n",
    "\n",
    "    def __len__(self): return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        tile, t = self.idx[i]\n",
    "        g = self.groups[tile]\n",
    "\n",
    "        # history window [t-L .. t-1]\n",
    "        hist = g.loc[t-self.L:t-1, self.feat_cols].values.astype(np.float32)   # [L, F]\n",
    "        \n",
    "        hist = ((hist - self.mean) / self.std).astype(np.float32)               # <- add .astype(np.float32)\n",
    "\n",
    "        # future targets [t .. t+H-1]\n",
    "        fut = g.loc[t:t+self.H-1, \"PM25_ug_m3\"].values.astype(np.float32)       # [H]\n",
    "\n",
    "        # metadata\n",
    "        start_ts = g.loc[t, \"timestamp\"]\n",
    "        lat = float(g[\"lat\"].iloc[0]); lon = float(g[\"lon\"].iloc[0])\n",
    "\n",
    "        return {\n",
    "            \"x_hist\": torch.from_numpy(hist),        # [L, F]\n",
    "            \"y_future\": torch.from_numpy(fut),       # [H]\n",
    "            \"tile_id\": tile,\n",
    "            \"start_time\": pd.Timestamp(start_ts),\n",
    "            \"lat\": lat, \"lon\": lon,\n",
    "        }\n",
    "\n",
    "# ---------- 6) Quick usage + rich debug ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load a CSV just for this run (swap to parquet reader when ready)\n",
    "    df_raw = pd.read_csv(\"data.csv\")  # expects: time, lat, lon, PM25_ug_m3 + numeric features\n",
    "\n",
    "    # Build tidy frame\n",
    "    df = prepare_dataframe(df_raw, hourly=True, dayfirst=True, freq=\"H\")\n",
    "\n",
    "    # Config (small so you see windows immediately)\n",
    "    L, H, stride = 8, 4, 1\n",
    "\n",
    "    # Feature stats\n",
    "    feat_cols, mean, std = compute_feature_stats(df)\n",
    "    F = len(feat_cols)\n",
    "\n",
    "    # Dataset\n",
    "    ds = TSWindowDataset(df, L=L, H=H, stride=stride, stats=(feat_cols, mean, std))\n",
    "\n",
    "    # ----------------- High-level summary -----------------\n",
    "    print(\"\\n=== DATA SUMMARY ===\")\n",
    "    print(\"df shape:\", df.shape)\n",
    "    print(\"time range:\", df[\"timestamp\"].min(), \"→\", df[\"timestamp\"].max())\n",
    "    print(\"#tiles:\", df[\"tile_id\"].nunique())\n",
    "    print(\"#features (F):\", F)\n",
    "    print(\"first 10 feature cols:\", feat_cols[:10])\n",
    "    print(f\"window config: L={L}, H={H}, stride={stride}\")\n",
    "    print(\"#windows:\", len(ds))\n",
    "\n",
    "    # Per-tile coverage (top/bottom few)\n",
    "    counts = df.groupby(\"tile_id\")[\"timestamp\"].count().sort_values()\n",
    "    print(\"\\nrows per tile (smallest 5):\")\n",
    "    print(counts.head(5))\n",
    "    print(\"rows per tile (largest 5):\")\n",
    "    print(counts.tail(5))\n",
    "\n",
    "    if len(ds) == 0:\n",
    "        raise SystemExit(\"\\n[!] No windows available. Increase coverage or lower L/H.\")\n",
    "\n",
    "    # ----------------- Inspect first window -----------------\n",
    "    print(\"\\n=== FIRST WINDOW DEBUG ===\")\n",
    "    tile0, t0 = ds.idx[0]\n",
    "    g0 = ds.groups[tile0]\n",
    "    b0 = ds[0]\n",
    "    x0, y0 = b0[\"x_hist\"], b0[\"y_future\"]\n",
    "\n",
    "    print(\"tile_id:\", tile0)\n",
    "    print(\"history shape [L,F]:\", tuple(x0.shape), \"| future shape [H]:\", tuple(y0.shape))\n",
    "\n",
    "    # time ranges for history & future\n",
    "    hist_ts = g0.loc[t0-L:t0-1, \"timestamp\"].to_list()\n",
    "    fut_ts  = g0.loc[t0:t0+H-1, \"timestamp\"].to_list()\n",
    "    print(\"hist timestamps:\", hist_ts[0], \"→\", hist_ts[-1])\n",
    "    print(\"fut  timestamps:\", fut_ts[0],  \"→\", fut_ts[-1])\n",
    "\n",
    "    # check spacing is hourly\n",
    "    hist_deltas = pd.Series(hist_ts).diff().dropna().dt.total_seconds().unique()\n",
    "    fut_deltas  = pd.Series(fut_ts).diff().dropna().dt.total_seconds().unique()\n",
    "    print(\"hist Δt seconds (unique):\", hist_deltas)\n",
    "    print(\"fut  Δt seconds (unique):\", fut_deltas)\n",
    "\n",
    "    # normalization sanity: mean ~ 0, std ~ 1 over this window (rough check)\n",
    "    print(\"x_hist window mean/std (overall):\", float(x0.mean()), float(x0.std()))\n",
    "    # show a single feature’s first few timesteps\n",
    "    print(\"x_hist[0:5, 0] sample:\", x0[:5, 0].tolist())\n",
    "    # show a few future targets\n",
    "    print(\"y_future sample:\", y0[:min(5, H)].tolist())\n",
    "\n",
    "    # ----------------- Batch check via DataLoader -----------------\n",
    "    loader = DataLoader(ds, batch_size=32, shuffle=True, drop_last=True,\n",
    "                    collate_fn=collate_keep_meta)\n",
    "\n",
    "    batch = next(iter(loader))\n",
    "    Xb, Yb = batch[\"x_hist\"], batch[\"y_future\"]\n",
    "    meta = batch[\"meta\"]\n",
    "\n",
    "    print(\"\\n=== BATCH CHECK (custom collate) ===\")\n",
    "    print(\"batch x_hist shape [B,L,F]:\", tuple(Xb.shape))\n",
    "    print(\"batch y_future shape [B,H]:\", tuple(Yb.shape))\n",
    "    print(\"meta keys:\", list(meta.keys()))\n",
    "    print(\"tile_id[0]:\", meta[\"tile_id\"][0])\n",
    "    print(\"start_time[0]:\", meta[\"start_time\"][0])\n",
    "    print(\"lat/lon tensors:\", tuple(meta[\"lat\"].shape), tuple(meta[\"lon\"].shape))\n",
    "\n",
    "    # ----------------- PatchTST token check (dev aid) -----------------\n",
    "    # If you plan to patchify later, this shows expected token count.\n",
    "    def patchify(x, P=16, S=8):  # x: [B,L,F] -> [B,N,P*F]\n",
    "        B,L_,F_ = x.shape\n",
    "        N = (L_ - P) // S + 1\n",
    "        return torch.stack([x[:, s:s+P, :].reshape(B, P*F_) for s in range(0, L_-P+1, S)], dim=1)\n",
    "\n",
    "    P, S = 4, 2  # small numbers just to visualize with L=8\n",
    "    tokens = patchify(Xb, P=P, S=S)\n",
    "    print(\"\\n=== PATCHIFY SMOKE TEST ===\")\n",
    "    print(f\"P={P}, S={S} -> N={(L-P)//S + 1}\")\n",
    "    print(\"tokens shape [B,N,P*F]:\", tuple(tokens.shape))\n",
    "\n",
    "    print(\"\\nAll sanity checks passed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293db734-eee5-4726-a993-8cb5584d95bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train windows: 5620 | #val windows: 100\n"
     ]
    }
   ],
   "source": [
    "# ---- time-based split (80/20 by timestamp) ----\n",
    "cutoff = df[\"timestamp\"].quantile(0.80)\n",
    "train_df = df[df[\"timestamp\"] <= cutoff].copy()\n",
    "val_df   = df[df[\"timestamp\"] >  cutoff].copy()\n",
    "\n",
    "# ---- stats on train only ----\n",
    "feat_cols, mean, std = compute_feature_stats(train_df)\n",
    "\n",
    "# ---- datasets ----\n",
    "L, H, stride = 168, 72, 1   # real config\n",
    "train_ds = TSWindowDataset(train_df, L=L, H=H, stride=stride, stats=(feat_cols, mean, std))\n",
    "val_ds   = TSWindowDataset(val_df,   L=L, H=H, stride=stride, stats=(feat_cols, mean, std))\n",
    "\n",
    "# ---- loaders (keep your custom collate) ----\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True, collate_fn=collate_keep_meta)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, drop_last=False, collate_fn=collate_keep_meta)\n",
    "\n",
    "print(\"#train windows:\", len(train_ds), \"| #val windows:\", len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03256ec7-98f3-4788-ac28-ecf55f032780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch.nn as nn, torch\n",
    "\n",
    "def sinusoidal_positional_encoding(n_pos: int, d_model: int, device=None):\n",
    "    pe = torch.zeros(n_pos, d_model, device=device)\n",
    "    pos = torch.arange(0, n_pos, device=device).unsqueeze(1).float()\n",
    "    div = torch.exp(torch.arange(0, d_model, 2, device=device).float() * (-math.log(10000.0)/d_model))\n",
    "    pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)\n",
    "    return pe  # [N, d]\n",
    "\n",
    "class PatchPosEncoder(nn.Module):\n",
    "    def __init__(self, in_features, patch_len=16, stride=8, d_model=128):\n",
    "        super().__init__()\n",
    "        self.P, self.S, self.F, self.d = patch_len, stride, in_features, d_model\n",
    "        self.proj = nn.Linear(self.P * self.F, self.d)\n",
    "\n",
    "    def forward(self, x):           # x: [B, L, F]\n",
    "        B, L, F = x.shape\n",
    "        starts = range(0, L - self.P + 1, self.S)\n",
    "        patches = [x[:, s:s+self.P, :].reshape(B, self.P*self.F) for s in starts]\n",
    "        T = torch.stack(patches, dim=1)              # [B, N, P*F]\n",
    "        T = self.proj(T)                             # [B, N, d]\n",
    "        pe = sinusoidal_positional_encoding(T.size(1), self.d, device=x.device)\n",
    "        return T + pe                                # [B, N, d]\n",
    "\n",
    "class SimplePatcherHead(nn.Module):\n",
    "    def __init__(self, in_features, L, H, patch_len=16, stride=8, d_model=128):\n",
    "        super().__init__()\n",
    "        self.enc = PatchPosEncoder(in_features, patch_len, stride, d_model)\n",
    "        self.head = nn.Linear(d_model, H)\n",
    "\n",
    "    def forward(self, x_hist):      # [B, L, F]\n",
    "        tokens = self.enc(x_hist)   # [B, N, d]\n",
    "        pooled = tokens[:, -1]      # last-token pool (or tokens.mean(dim=1)) \n",
    "        \n",
    "        return self.head(pooled)    # [B, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020c4a00-8d21-4aa6-a65a-4fc77032cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat: torch.Size([32, 72])\n"
     ]
    }
   ],
   "source": [
    "F = len(feat_cols)\n",
    "model = SimplePatcherHead(in_features=F, L=L, H=H, patch_len=16, stride=8, d_model=128)\n",
    "batch = next(iter(train_loader))\n",
    "x, y = batch[\"x_hist\"], batch[\"y_future\"]\n",
    "y_hat = model(x)\n",
    "print(\"y_hat:\", y_hat.shape)  # [B, 72]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e009d0aa-afb3-4268-9a58-aaa305f63172",
   "metadata": {},
   "source": [
    "# TESTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44ac8d-f815-4e54-a46a-9de96d0ca2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torch.nn as nn\n",
    "import pennylane as qml\n",
    "\n",
    "class QuantumEmbedOnly(nn.Module):\n",
    "    \"\"\"\n",
    "    Data-only embedding: angles = Linear(d_model -> n_qubits)\n",
    "    Circuit: RY(angle_i) on each qubit, measure <Z_i> for all wires.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, n_qubits: int = 8):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.angle_proj = nn.Linear(d_model, n_qubits)\n",
    "\n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=\"parameter-shift\")\n",
    "        def circuit(angles_1d):\n",
    "            for i in range(n_qubits):\n",
    "                qml.RY(angles_1d[i], wires=i)\n",
    "            # per-qubit Z expectation values\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "        self.circuit = circuit\n",
    "\n",
    "    def forward(self, pooled):  # pooled: [B, d_model], float32\n",
    "        angles = torch.tanh(self.angle_proj(pooled)) * math.pi      # [B, n_qubits], float32\n",
    "\n",
    "        outs = []\n",
    "        for b in range(angles.shape[0]):\n",
    "            o = self.circuit(angles[b].double())                    # list/tuple OR tensor\n",
    "            if isinstance(o, (list, tuple)):\n",
    "                o = torch.stack([oi if isinstance(oi, torch.Tensor)\n",
    "                                  else torch.as_tensor(oi, dtype=torch.float64)\n",
    "                                  for oi in o], dim=0)\n",
    "            outs.append(o)\n",
    "\n",
    "        qfeat = torch.stack(outs, dim=0).float()                    # [B, n_qubits], float32\n",
    "        return qfeat\n",
    "\n",
    "class PatchToQuantum72(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch+PosEnc (yours) -> last-token pool -> QuantumEmbedOnly -> Linear -> 72\n",
    "    No changes to your earlier blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, L: int, H: int,\n",
    "                 patch_len=16, stride=8, d_model=128, n_qubits=8):\n",
    "        super().__init__()\n",
    "        self.enc   = PatchPosEncoder(in_features, patch_len, stride, d_model)  # encoder\n",
    "        self.qemb  = QuantumEmbedOnly(d_model=d_model, n_qubits=n_qubits)      # quantum embedding only\n",
    "        self.head  = nn.Linear(n_qubits, H)\n",
    "\n",
    "    def forward(self, x_hist):            # x_hist: [B, L, F] \n",
    "        tokens = self.enc(x_hist)         # [B, N, d_model]\n",
    "        # pooled = tokens[:, -1]            # [B, d_model]  (or tokens.mean(dim=1)) (or\n",
    "        \n",
    "        q_each = [self.qemb(tokens[:, i, :]) for i in range(tokens.shape[1])]  # list of [B, n_qubits]\n",
    "        q_stack = torch.stack(q_each, dim=1)  # [B, N, n_qubits]\n",
    "        qfeat = q_stack.mean(dim=1)           # [B, n_qubits]  (or attention here too)\n",
    "\n",
    "        # qfeat  = self.qemb(pooled)        # [B, n_qubits]\n",
    "        return self.head(qfeat)           # [B, 72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb10df51-e17f-4fc4-bb09-b38dfa10c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat shape: torch.Size([32, 72])\n",
      "loss: 2478.3671875\n"
     ]
    }
   ],
   "source": [
    "F = len(feat_cols)\n",
    "model = PatchToQuantum72(in_features=F, L=L, H=H, patch_len=16, stride=8,\n",
    "                         d_model=128, n_qubits=8)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "x, y = batch[\"x_hist\"], batch[\"y_future\"]         # x:[B,L,F], y:[B,72]\n",
    "y_hat = model(x)                                  # -> [B,72]\n",
    "print(\"y_hat shape:\", y_hat.shape)\n",
    "\n",
    "# quick gradient check\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss = nn.MSELoss()(y_hat, y)\n",
    "opt.zero_grad(); loss.backward(); opt.step()\n",
    "print(\"loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a17b8-9505-40d3-a2b7-f2a0732191f3",
   "metadata": {},
   "source": [
    "# QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf17d76-469a-4d36-b21f-5153e34a962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === QLSTM cell + wrapper that uses your PatchPosEncoder ===\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class zzfeatuermapQLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantum LSTM cell:\n",
    "      concat([h_t, x_t]) -> Linear -> n_qubits\n",
    "      -> IQPEmbedding + BasicEntanglerLayers -> Z expvals\n",
    "      -> Linear -> hidden_size\n",
    "      -> standard LSTM updates\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, n_qubits=4, n_qlayers=1, backend=\"default.qubit\"):\n",
    "        super().__init__()\n",
    "        self.input_size  = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = input_size + hidden_size\n",
    "        self.n_qubits    = n_qubits\n",
    "        self.n_qlayers   = n_qlayers\n",
    "\n",
    "        # Separate devices per gate\n",
    "        self.dev_forget = qml.device(backend, wires=n_qubits)\n",
    "        self.dev_input  = qml.device(backend, wires=n_qubits)\n",
    "        self.dev_update = qml.device(backend, wires=n_qubits)\n",
    "        self.dev_output = qml.device(backend, wires=n_qubits)\n",
    "\n",
    "        # Gate circuits (same topology for each)  <-- NOTE the arg name: inputs\n",
    "        def _circuit_forget(inputs, weights):\n",
    "            qml.templates.IQPEmbedding(inputs, wires=range(n_qubits))\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "        def _circuit_input(inputs, weights):\n",
    "            qml.templates.IQPEmbedding(inputs, wires=range(n_qubits))\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "        def _circuit_update(inputs, weights):\n",
    "            qml.templates.IQPEmbedding(inputs, wires=range(n_qubits))\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "        def _circuit_output(inputs, weights):\n",
    "            qml.templates.IQPEmbedding(inputs, wires=range(n_qubits))\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "        weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        # Older TorchLayer: just pass the QNode and weight_shapes\n",
    "        self.qlayer_forget = qml.qnn.TorchLayer(\n",
    "            qml.QNode(_circuit_forget, self.dev_forget, interface=\"torch\"),\n",
    "            weight_shapes\n",
    "        )\n",
    "        self.qlayer_input  = qml.qnn.TorchLayer(\n",
    "            qml.QNode(_circuit_input,  self.dev_input,  interface=\"torch\"),\n",
    "            weight_shapes\n",
    "        )\n",
    "        self.qlayer_update = qml.qnn.TorchLayer(\n",
    "            qml.QNode(_circuit_update, self.dev_update, interface=\"torch\"),\n",
    "            weight_shapes\n",
    "        )\n",
    "        self.qlayer_output = qml.qnn.TorchLayer(\n",
    "            qml.QNode(_circuit_output, self.dev_output, interface=\"torch\"),\n",
    "            weight_shapes\n",
    "        )\n",
    "\n",
    "        # Classical pre/post\n",
    "        self.clayer_in  = nn.Linear(self.concat_size, n_qubits)   # [h_t, x_t] -> n_qubits\n",
    "        self.clayer_out = nn.Linear(n_qubits, hidden_size)        # Z-expvals -> hidden_size\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        x: [B, N, input_size]  (your Patch+PosEnc tokens)\n",
    "        returns: hidden_seq [B, N, hidden], (h_T, c_T)\n",
    "        \"\"\"\n",
    "        B, N, _ = x.size()\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(B, self.hidden_size, device=x.device, dtype=x.dtype)\n",
    "            c_t = torch.zeros(B, self.hidden_size, device=x.device, dtype=x.dtype)\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "\n",
    "        h_list = []\n",
    "        for t in range(N):\n",
    "            x_t = x[:, t, :]                         # [B, input_size]\n",
    "            v_t = torch.cat([h_t, x_t], dim=1)       # [B, hidden+input]\n",
    "            y_t = self.clayer_in(v_t)                # [B, n_qubits]\n",
    "\n",
    "            f_t = torch.sigmoid(self.clayer_out(self.qlayer_forget(y_t)))  # [B, hidden]\n",
    "            i_t = torch.sigmoid(self.clayer_out(self.qlayer_input (y_t)))  # [B, hidden]\n",
    "            g_t = torch.tanh   (self.clayer_out(self.qlayer_update(y_t)))  # [B, hidden]\n",
    "            o_t = torch.sigmoid(self.clayer_out(self.qlayer_output(y_t)))  # [B, hidden]\n",
    "\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            h_list.append(h_t.unsqueeze(1))\n",
    "\n",
    "        hidden_seq = torch.cat(h_list, dim=1)        # [B, N, hidden]\n",
    "        return hidden_seq, (h_t, c_t)\n",
    "\n",
    "\n",
    "class PatchToQLSTM72(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchPosEncoder (yours) -> QLSTM over tokens -> Linear -> H (e.g., 72).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, L, H, patch_len=16, stride=8,\n",
    "                 d_model=128, hidden_size=128, n_qubits=4, n_qlayers=1, backend=\"default.qubit\"):\n",
    "        super().__init__()\n",
    "        self.enc   = PatchPosEncoder(in_features, patch_len, stride, d_model)  # uses your existing class\n",
    "        self.qlstm = zzfeatuermapQLSTM(input_size=d_model, hidden_size=hidden_size,\n",
    "                                       n_qubits=n_qubits, n_qlayers=n_qlayers, backend=backend)\n",
    "        self.head  = nn.Linear(hidden_size, H)\n",
    "\n",
    "    def forward(self, x_hist):               # x_hist: [B, L, F]\n",
    "        tokens = self.enc(x_hist)            # [B, N, d_model]\n",
    "        h_seq, (hT, cT) = self.qlstm(tokens) # [B, N, hidden]\n",
    "        return self.head(h_seq[:, -1, :])    # [B, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de255f4-f99e-44c8-be18-7a6e9f1d96da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLSTM model built and forward pass OK.\n"
     ]
    }
   ],
   "source": [
    "# --- Build the QLSTM model (must run before the training loop) ---\n",
    "F = len(feat_cols)  # number of features from your dataframe pipeline\n",
    "\n",
    "# If you have lightning backends installed you can swap \"default.qubit\" -> \"lightning.qubit\" (CPU) or \"lightning.gpu\"\n",
    "backend = \"default.qubit\"\n",
    "\n",
    "qlstm_model = PatchToQLSTM72(\n",
    "    in_features=F, L=L, H=H,\n",
    "    patch_len=16, stride=8,\n",
    "    d_model=128, hidden_size=128,\n",
    "    n_qubits=4, n_qlayers=1,\n",
    "    backend=backend\n",
    ")\n",
    "\n",
    "# (optional) quick smoke test so we fail early if shapes don’t match\n",
    "with torch.no_grad():\n",
    "    _ = qlstm_model(next(iter(train_loader))[\"x_hist\"][:2])\n",
    "print(\"QLSTM model built and forward pass OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3eb82e-2439-4bed-a9ab-66c9642decee",
   "metadata": {},
   "source": [
    "# QGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0991ea-c5da-47ce-ae75-ed38ef838974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === QGRU cell + wrapper that uses your PatchPosEncoder (drop-in alongside QLSTM) ===\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class zzfeatuermapQGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantum GRU cell:\n",
    "      Gates (reset/update/new) are VQCs.\n",
    "      - For r,z gates: concat([h_t, x_t]) -> Linear -> n_qubits -> QNode -> Linear -> hidden_size\n",
    "      - For candidate n_t: concat([r_t * h_t, x_t]) -> Linear -> n_qubits -> QNode -> Linear -> hidden_size\n",
    "      - Standard GRU update: h_t = (1 - z_t) * n_t + z_t * h_{t-1}\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, n_qubits=4, n_qlayers=1, backend=\"default.qubit\"):\n",
    "        super().__init__()\n",
    "        self.n_inputs = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.backend = backend\n",
    "\n",
    "        # unique wire names per gate (separate devices)\n",
    "        self.wires_reset  = [f\"wire_reset_{i}\"  for i in range(self.n_qubits)]\n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_new    = [f\"wire_new_{i}\"    for i in range(self.n_qubits)]\n",
    "\n",
    "        self.dev_reset  = qml.device(self.backend, wires=self.wires_reset)\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\n",
    "        self.dev_new    = qml.device(self.backend, wires=self.wires_new)\n",
    "\n",
    "        # circuits: IQPEmbedding + BasicEntanglerLayers -> Z expvals\n",
    "        def _circuit_reset(inputs, weights):\n",
    "            qml.templates.IQPEmbedding(inputs, wires=self.wires_reset)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_reset)\n",
    "            return [qml.expval(qml.PauliZ(w)) for w in self.wires_reset]\n",
    "\n",
    "        def _circuit_update(inputs, weights):\n",
    "            qml.templates.IQPEmbedding(inputs, wires=self.wires_update)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_update)\n",
    "            return [qml.expval(qml.PauliZ(w)) for w in self.wires_update]\n",
    "\n",
    "        def _circuit_new(inputs, weights):\n",
    "            qml.templates.IQPEmbedding(inputs, wires=self.wires_new)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_new)\n",
    "            return [qml.expval(qml.PauliZ(w)) for w in self.wires_new]\n",
    "\n",
    "        weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        self.qlayer_reset  = qml.qnn.TorchLayer(qml.QNode(_circuit_reset,  self.dev_reset,  interface=\"torch\"), weight_shapes)\n",
    "        self.qlayer_update = qml.qnn.TorchLayer(qml.QNode(_circuit_update, self.dev_update, interface=\"torch\"), weight_shapes)\n",
    "        self.qlayer_new    = qml.qnn.TorchLayer(qml.QNode(_circuit_new,    self.dev_new,    interface=\"torch\"), weight_shapes)\n",
    "\n",
    "        # classical pre/post\n",
    "        self.clayer_in_gates = nn.Linear(self.concat_size, n_qubits)  # for r,z\n",
    "        self.clayer_in_cand  = nn.Linear(self.concat_size, n_qubits)  # for candidate n with (r ⊙ h)\n",
    "        self.clayer_out      = nn.Linear(self.n_qubits, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, init_state=None):\n",
    "        \"\"\"\n",
    "        x: [B, N, input_size]  tokens from Patch+PosEnc\n",
    "        returns: hidden_seq [B, N, hidden], h_T\n",
    "        \"\"\"\n",
    "        B, N, _ = x.size()\n",
    "        hidden_seq = []\n",
    "\n",
    "        if init_state is None:\n",
    "            h_t = torch.zeros(B, self.hidden_size, device=x.device, dtype=x.dtype)\n",
    "        else:\n",
    "            h_t = init_state\n",
    "\n",
    "        for t in range(N):\n",
    "            x_t = x[:, t, :]                              # [B, input_size]\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)            # [B, hidden+input]\n",
    "            y_t = self.clayer_in_gates(v_t)               # [B, n_qubits]\n",
    "\n",
    "            r_t = torch.sigmoid(self.clayer_out(self.qlayer_reset(y_t)))   # [B, hidden]\n",
    "            z_t = torch.sigmoid(self.clayer_out(self.qlayer_update(y_t)))  # [B, hidden]\n",
    "\n",
    "            v_cand = torch.cat((r_t * h_t, x_t), dim=1)   # [B, hidden+input]\n",
    "            y_cand = self.clayer_in_cand(v_cand)          # [B, n_qubits]\n",
    "            n_t = torch.tanh(self.clayer_out(self.qlayer_new(y_cand)))     # [B, hidden]\n",
    "\n",
    "            h_t = (1.0 - z_t) * n_t + z_t * h_t\n",
    "            hidden_seq.append(h_t.unsqueeze(1))\n",
    "\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=1)         # [B, N, hidden]\n",
    "        return hidden_seq, h_t\n",
    "\n",
    "\n",
    "class PatchToQGRU72(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchPosEncoder (yours) -> QGRU over tokens -> Linear -> H (e.g., 72).\n",
    "    Same interface as PatchToQLSTM72 so you can swap easily.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, L, H, patch_len=16, stride=8,\n",
    "                 d_model=128, hidden_size=128, n_qubits=4, n_qlayers=1, backend=\"default.qubit\"):\n",
    "        super().__init__()\n",
    "        self.enc  = PatchPosEncoder(in_features, patch_len, stride, d_model)\n",
    "        self.qgru = zzfeatuermapQGRU(input_size=d_model, hidden_size=hidden_size,\n",
    "                                     n_qubits=n_qubits, n_qlayers=n_qlayers, backend=backend)\n",
    "        self.head = nn.Linear(hidden_size, H)\n",
    "\n",
    "    def forward(self, x_hist):                # x_hist: [B, L, F]\n",
    "        tokens = self.enc(x_hist)             # [B, N, d_model]\n",
    "        h_seq, hT = self.qgru(tokens)         # [B, N, hidden], [B, hidden]\n",
    "        return self.head(h_seq[:, -1, :])     # [B, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56898146-91ae-4678-86cf-4d059f60105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QGRU model built and forward pass OK.\n"
     ]
    }
   ],
   "source": [
    "# reuse your existing values: F=len(feat_cols), L, H, backend, train_loader\n",
    "F = len(feat_cols)  # number of features from your dataframe pipeline\n",
    "\n",
    "# If you have lightning backends installed you can swap \"default.qubit\" -> \"lightning.qubit\" (CPU) or \"lightning.gpu\"\n",
    "backend = \"default.qubit\"\n",
    "\n",
    "qgru_model = PatchToQGRU72(\n",
    "    in_features=F, L=L, H=H,\n",
    "    patch_len=16, stride=8,\n",
    "    d_model=128, hidden_size=128,\n",
    "    n_qubits=4, n_qlayers=1,\n",
    "    backend=backend\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = qgru_model(next(iter(train_loader))[\"x_hist\"][:2])\n",
    "print(\"QGRU model built and forward pass OK.\")\n",
    "\n",
    "# If you want to train QGRU **without touching your training loop** that references `qlstm_model`,\n",
    "# simply alias it:\n",
    "qlstm_model = qgru_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372132d-2ade-4e42-9056-a3d38c170487",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c4ec7c2-5b42-43d8-a8dd-31cd5c78f179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu for 30 epochs...\n",
      "Epoch 001 | train 2386.8321 (143.7s) | val 1973.1548 (2.0s) | lr 1.00e-03\n",
      "Epoch 002 | train 1754.0602 (143.0s) | val 1386.1457 (2.1s) | lr 9.97e-04\n",
      "Epoch 003 | train 1154.2798 (144.2s) | val 951.5430 (2.1s) | lr 9.89e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Train QLSTM for multiple epochs and plot the loss ===\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- config (tweak as you like) ---\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "GRAD_CLIP = 1.0  # helps stabilize PQC training\n",
    "PRINT_EVERY = 1\n",
    "\n",
    "# Pennylane's default.qubit + TorchLayer are CPU-friendly; keeping everything on CPU avoids device mismatches\n",
    "device = torch.device(\"cpu\")\n",
    "qlstm_model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(qlstm_model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "# optional cosine decay\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "def run_epoch(model, loader, train: bool):\n",
    "    model.train(train)\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    start = time.time()\n",
    "    for batch in loader:\n",
    "        x = batch[\"x_hist\"].to(device)          # [B, L, F]\n",
    "        y = batch[\"y_future\"].to(device)        # [B, H]\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(train):\n",
    "            y_hat = model(x)                    # [B, H]\n",
    "            loss = criterion(y_hat, y)\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                if GRAD_CLIP is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "                optimizer.step()\n",
    "        total += float(loss) * x.size(0)\n",
    "        count += x.size(0)\n",
    "    elapsed = time.time() - start\n",
    "    return total / max(count, 1), elapsed\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "lrs = []\n",
    "\n",
    "print(f\"Training on {device} for {EPOCHS} epochs...\")\n",
    "best_val = math.inf\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_time = run_epoch(qlstm_model, train_loader, train=True)\n",
    "    va_loss, va_time = run_epoch(qlstm_model, val_loader,   train=False)\n",
    "    train_losses.append(tr_loss)\n",
    "    val_losses.append(va_loss)\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in qlstm_model.state_dict().items()}\n",
    "\n",
    "    if epoch % PRINT_EVERY == 0:\n",
    "        print(f\"Epoch {epoch:03d} | \"\n",
    "              f\"train {tr_loss:.4f} ({tr_time:.1f}s) | \"\n",
    "              f\"val {va_loss:.4f} ({va_time:.1f}s) | \"\n",
    "              f\"lr {lrs[-1]:.2e}\")\n",
    "\n",
    "# (optional) load best\n",
    "if best_state is not None:\n",
    "    qlstm_model.load_state_dict(best_state)\n",
    "    print(f\"\\nLoaded best model (val MSE = {best_val:.4f}).\")\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.title(\"QLSTM: training and validation loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (optional) plot learning rate\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.plot(lrs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"learning rate\")\n",
    "plt.title(\"Learning rate schedule\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad4334-e7b4-4267-8b83-436571922498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenva)",
   "language": "python",
   "name": "myenva"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
