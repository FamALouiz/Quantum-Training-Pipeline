{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6b0572-592f-4e37-93c3-88f05f365715",
   "metadata": {},
   "source": [
    "# BLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8387dd-8b1d-481c-aef7-b99e98438e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLS_config.py\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional   # <-- add this for Python 3.9\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BLSConfig:\n",
    "    n_feature_groups: int = 10\n",
    "    feature_group_size: int = 10\n",
    "    n_enhancement_groups: int = 10\n",
    "    enhancement_group_size: int = 10\n",
    "    feature_activation: str = \"tanh\"          # identity, tanh, sigmoid, relu\n",
    "    enhancement_activation: str = \"tanh\"      # identity, tanh, sigmoid, relu\n",
    "    lambda_reg: float = 1e-2\n",
    "    add_bias: bool = True\n",
    "    standardize: bool = True\n",
    "    random_state: Optional[int] = 42          # <-- changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f6a725-604f-4a6a-a332-86432146bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLS.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.utils.extmath import softmax\n",
    "\n",
    "\n",
    "_ACTS = {\n",
    "    \"identity\": lambda x: x,\n",
    "    \"tanh\": np.tanh,\n",
    "    \"sigmoid\": lambda x: 1.0 / (1.0 + np.exp(-x)),\n",
    "    \"relu\": lambda x: np.maximum(0.0, x),\n",
    "}\n",
    "\n",
    "\n",
    "class BroadLearningSystem:\n",
    "    \"\"\"\n",
    "    Broad Learning System that uses scikit-learn:\n",
    "      - StandardScaler\n",
    "      - OneHotEncoder\n",
    "      - Ridge for closed-form output weights\n",
    "      - train_test_split helper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: BLSConfig):\n",
    "        \"\"\"\n",
    "        Initialize the Broad Learning System.\n",
    "\n",
    "        Args:\n",
    "            cfg (BLSConfig): Configuration object containing all hyperparameters\n",
    "                for the BLS model including feature groups, enhancement groups,\n",
    "                activation functions, and regularization parameters.\n",
    "        \"\"\"\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.default_rng(cfg.random_state)\n",
    "        self.scaler = StandardScaler(\n",
    "            with_mean=True, with_std=True) if cfg.standardize else None\n",
    "        self.enc = None\n",
    "        self.is_classification = None\n",
    "        self.classes_ = None\n",
    "\n",
    "        self.Wf, self.bf = [], []\n",
    "        self.We, self.be = [], []\n",
    "        self.Wout = None\n",
    "        self.ridge = None\n",
    "\n",
    "        if cfg.feature_activation not in _ACTS or cfg.enhancement_activation not in _ACTS:\n",
    "            raise ValueError(\"Unknown activation name.\")\n",
    "        self.act_f = _ACTS[cfg.feature_activation]\n",
    "        self.act_e = _ACTS[cfg.enhancement_activation]\n",
    "\n",
    "    @staticmethod\n",
    "    def split(X, y, test_size=0.2, random_state=0, stratify=None):\n",
    "        \"\"\"\n",
    "        Split arrays or matrices into random train and test subsets.\n",
    "\n",
    "        This is a convenience wrapper around sklearn's train_test_split.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): Features array of shape (n_samples, n_features).\n",
    "            y (array-like): Target array of shape (n_samples,) or (n_samples, n_outputs).\n",
    "            test_size (float, optional): Proportion of dataset to include in test split. \n",
    "                Defaults to 0.2.\n",
    "            random_state (int, optional): Random seed for reproducible splits. \n",
    "                Defaults to 0.\n",
    "            stratify (array-like, optional): If not None, data is split in a stratified \n",
    "                fashion using this as class labels. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train, X_test, y_train, y_test arrays.\n",
    "        \"\"\"\n",
    "        return train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=stratify)\n",
    "\n",
    "    # ---------- internal ----------\n",
    "    def _init_groups(self, in_dim):\n",
    "        \"\"\"\n",
    "        Initialize feature and enhancement groups with random weights and biases.\n",
    "\n",
    "        Creates random weight matrices and bias vectors for both feature mapping\n",
    "        groups and enhancement groups. Feature groups map input to feature space,\n",
    "        while enhancement groups map feature outputs to enhancement space.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Input dimension for feature groups.\n",
    "        \"\"\"\n",
    "        self.Wf, self.bf = [], []\n",
    "        for _ in range(self.cfg.n_feature_groups):\n",
    "            W = self.rng.normal(0.0, 1.0, size=(\n",
    "                in_dim, self.cfg.feature_group_size))\n",
    "            b = self.rng.normal(0.0, 0.1, size=(self.cfg.feature_group_size,))\n",
    "            self.Wf.append(W)\n",
    "            self.bf.append(b)\n",
    "\n",
    "        total_feature_nodes = self.cfg.n_feature_groups * self.cfg.feature_group_size\n",
    "        self.We, self.be = [], []\n",
    "        for _ in range(self.cfg.n_enhancement_groups):\n",
    "            W = self.rng.normal(0.0, 1.0, size=(\n",
    "                total_feature_nodes, self.cfg.enhancement_group_size))\n",
    "            b = self.rng.normal(0.0, 0.1, size=(\n",
    "                self.cfg.enhancement_group_size,))\n",
    "            self.We.append(W)\n",
    "            self.be.append(b)\n",
    "\n",
    "    def _map_features(self, X):\n",
    "        \"\"\"\n",
    "        Map input data through feature groups using random weights and activation functions.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Feature mappings of shape (n_samples, n_feature_groups * feature_group_size).\n",
    "        \"\"\"\n",
    "        feats = [self.act_f(X @ W + b) for W, b in zip(self.Wf, self.bf)]\n",
    "        return np.concatenate(feats, axis=1) if feats else np.empty((X.shape[0], 0))\n",
    "\n",
    "    def _map_enhancements(self, F):\n",
    "        \"\"\"\n",
    "        Map feature outputs through enhancement groups using random weights and activation functions.\n",
    "\n",
    "        Args:\n",
    "            F (np.ndarray): Feature mappings of shape (n_samples, n_feature_nodes).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Enhancement mappings of shape (n_samples, n_enhancement_groups * enhancement_group_size).\n",
    "        \"\"\"\n",
    "        enh = [self.act_e(F @ W + b) for W, b in zip(self.We, self.be)]\n",
    "        return np.concatenate(enh, axis=1) if enh else np.empty((F.shape[0], 0))\n",
    "\n",
    "    def _design(self, X):\n",
    "        \"\"\"\n",
    "        Create the design matrix by concatenating feature and enhancement mappings.\n",
    "\n",
    "        Applies feature mapping, enhancement mapping, and optionally adds bias term\n",
    "        to create the final design matrix used for output weight learning.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Design matrix of shape (n_samples, total_nodes + bias).\n",
    "        \"\"\"\n",
    "        F = self._map_features(X)\n",
    "        E = self._map_enhancements(F)\n",
    "        H = np.concatenate([F, E], axis=1) if E.size else F\n",
    "        if self.cfg.add_bias:\n",
    "            H = np.concatenate([H, np.ones((H.shape[0], 1))], axis=1)\n",
    "        return H\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Broad Learning System on the given data.\n",
    "\n",
    "        Fits the model by standardizing input, initializing random groups,\n",
    "        preparing target encoding (for classification), creating design matrix,\n",
    "        and solving for optimal output weights using Ridge regression.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): Training features of shape (n_samples, n_features).\n",
    "            y (array-like): Training targets of shape (n_samples,) or (n_samples, n_outputs).\n",
    "\n",
    "        Returns:\n",
    "            BroadLearningSystem: Returns self for method chaining.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=float)\n",
    "\n",
    "        if self.scaler:\n",
    "            Xs = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            Xs = X\n",
    "\n",
    "        self._init_groups(Xs.shape[1])\n",
    "\n",
    "        y = np.asarray(y)\n",
    "        if y.ndim == 1 or (y.ndim == 2 and y.shape[1] == 1):\n",
    "            self.is_classification = True\n",
    "            y = y.reshape(-1, 1)\n",
    "            self.enc = OneHotEncoder(\n",
    "                sparse_output=False, handle_unknown=\"ignore\")\n",
    "            T = self.enc.fit_transform(y)\n",
    "            self.classes_ = self.enc.categories_[0]\n",
    "        else:\n",
    "            self.is_classification = False\n",
    "            T = y.astype(float)\n",
    "\n",
    "        H = self._design(Xs)\n",
    "        self.ridge = Ridge(alpha=self.cfg.lambda_reg,\n",
    "                           fit_intercept=False, random_state=self.cfg.random_state)\n",
    "        self.ridge.fit(H, T)\n",
    "        self.Wout = self.ridge.coef_.T\n",
    "        return self\n",
    "\n",
    "    def _forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward pass through the trained BLS model.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Model outputs of shape (n_samples, n_outputs).\n",
    "        \"\"\"\n",
    "        if self.scaler:\n",
    "            X = self.scaler.transform(X)\n",
    "        H = self._design(X)\n",
    "        return H @ self.Wout\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "\n",
    "        For classification tasks, returns predicted class labels.\n",
    "        For regression tasks, returns predicted continuous values.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): Input features of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predictions of shape (n_samples,) for classification or \n",
    "                       (n_samples, n_outputs) for regression.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        Y = self._forward(X)\n",
    "        if self.is_classification:\n",
    "            idx = np.argmax(Y, axis=1)\n",
    "            return self.classes_[idx]\n",
    "        return Y\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute class probabilities for input samples.\n",
    "\n",
    "        Only available for classification tasks. Uses softmax to convert\n",
    "        logits to probability distributions.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): Input features of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Class probabilities of shape (n_samples, n_classes).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If called on a regression model.\n",
    "        \"\"\"\n",
    "        if not self.is_classification:\n",
    "            raise ValueError(\"predict_proba only for classification.\")\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        logits = self._forward(X)\n",
    "        return softmax(logits)\n",
    "\n",
    "    def add_feature_groups(self, k):\n",
    "        \"\"\"\n",
    "        Add additional feature groups to expand the network breadth.\n",
    "\n",
    "        This method supports incremental learning by adding new feature mapping\n",
    "        groups without retraining the entire model. The output weights will need\n",
    "        to be refitted after expansion.\n",
    "\n",
    "        Args:\n",
    "            k (int): Number of feature groups to add.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If called before the model is fitted.\n",
    "        \"\"\"\n",
    "        in_dim = self.scaler.mean_.shape[0] if self.scaler else None\n",
    "        if in_dim is None:\n",
    "            raise RuntimeError(\"Model must be fitted before adding groups.\")\n",
    "        for _ in range(k):\n",
    "            W = self.rng.normal(0.0, 1.0, size=(\n",
    "                in_dim, self.cfg.feature_group_size))\n",
    "            b = self.rng.normal(0.0, 0.1, size=(self.cfg.feature_group_size,))\n",
    "            self.Wf.append(W)\n",
    "            self.bf.append(b)\n",
    "\n",
    "    def add_enhancement_groups(self, k):\n",
    "        \"\"\"\n",
    "        Add additional enhancement groups to expand the network breadth.\n",
    "\n",
    "        This method supports incremental learning by adding new enhancement\n",
    "        groups that operate on the current feature space. The output weights\n",
    "        will need to be refitted after expansion.\n",
    "\n",
    "        Args:\n",
    "            k (int): Number of enhancement groups to add.\n",
    "        \"\"\"\n",
    "        total_feature_nodes = len(self.Wf) * self.cfg.feature_group_size\n",
    "        for _ in range(k):\n",
    "            W = self.rng.normal(0.0, 1.0, size=(\n",
    "                total_feature_nodes, self.cfg.enhancement_group_size))\n",
    "            b = self.rng.normal(0.0, 0.1, size=(\n",
    "                self.cfg.enhancement_group_size,))\n",
    "            self.We.append(W)\n",
    "            self.be.append(b)\n",
    "\n",
    "    def refit_output(self, X, y):\n",
    "        \"\"\"\n",
    "        Refit only the output weights using the current network architecture.\n",
    "\n",
    "        This method is efficient for updating the model after adding new feature\n",
    "        or enhancement groups, as it only recomputes the final Ridge regression\n",
    "        without reinitializing the random groups.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): Training features of shape (n_samples, n_features).\n",
    "            y (array-like): Training targets of shape (n_samples,) or (n_samples, n_outputs).\n",
    "\n",
    "        Returns:\n",
    "            BroadLearningSystem: Returns self for method chaining.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        if self.scaler:\n",
    "            X = self.scaler.transform(X)\n",
    "\n",
    "        y = np.asarray(y)\n",
    "        if self.is_classification:\n",
    "            T = self.enc.transform(y.reshape(-1, 1))\n",
    "        else:\n",
    "            T = y.astype(float)\n",
    "\n",
    "        H = self._design(X)\n",
    "        self.ridge.fit(H, T)\n",
    "        self.Wout = self.ridge.coef_.T\n",
    "        return self\n",
    "\n",
    "    def extract_features(self, X):\n",
    "        \"\"\"\n",
    "        Extract the intermediate feature representation from the BLS network.\n",
    "\n",
    "        Returns the design matrix (concatenated feature and enhancement mappings)\n",
    "        without applying the final output weights. Useful for feature extraction\n",
    "        and as input to other models like LSTMs.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): Input features of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Feature representation of shape (n_samples, total_nodes + bias).\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        if self.scaler:\n",
    "            X = self.scaler.transform(X)\n",
    "        return self._design(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b812cca-42b5-41eb-8465-09cd526a150a",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386b79d-4ad1-4bbc-8698-f4ef555ac255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA SUMMARY ===\n",
      "df shape: (24400, 55)\n",
      "time range: 2021-01-10 00:00:00 → 2021-02-10 19:00:00\n",
      "#tiles: 400\n",
      "#features (F): 50\n",
      "first 10 feature cols: ['DUEXTTAU', 'BCFLUXU', 'OCFLUXV', 'BCANGSTR', 'SUFLUXV', 'SSSMASS25', 'SSSMASS', 'OCSMASS', 'BCCMASS', 'BCSMASS']\n",
      "window config: L=8, H=4, stride=1\n",
      "#windows: 20000\n",
      "\n",
      "rows per tile (smallest 5):\n",
      "tile_id\n",
      "26.5_36.875    24\n",
      "28.5_30.625    24\n",
      "28.5_30.0      24\n",
      "28.5_29.375    24\n",
      "28.5_28.75     24\n",
      "Name: timestamp, dtype: int64\n",
      "rows per tile (largest 5):\n",
      "tile_id\n",
      "31.0_25.0    764\n",
      "26.0_25.0    764\n",
      "27.5_25.0    764\n",
      "25.5_25.0    764\n",
      "22.0_25.0    764\n",
      "Name: timestamp, dtype: int64\n",
      "\n",
      "=== FIRST WINDOW DEBUG ===\n",
      "tile_id: 22.0_25.0\n",
      "history shape [L,F]: (8, 50) | future shape [H]: (4,)\n",
      "hist timestamps: 2021-01-10 00:00:00 → 2021-01-10 07:00:00\n",
      "fut  timestamps: 2021-01-10 08:00:00 → 2021-01-10 11:00:00\n",
      "hist Δt seconds (unique): [3600.]\n",
      "fut  Δt seconds (unique): [3600.]\n",
      "x_hist window mean/std (overall): -0.3407861292362213 0.7213065028190613\n",
      "x_hist[0:5, 0] sample: [0.03592585772275925, 0.04441426694393158, 0.05713042989373207, 0.0698879063129425, 0.09415498375892639]\n",
      "y_future sample: [26.396438598632812, 25.872926712036133, 26.150850296020508, 27.159780502319336]\n",
      "\n",
      "=== BATCH CHECK (custom collate) ===\n",
      "batch x_hist shape [B,L,F]: (32, 8, 50)\n",
      "batch y_future shape [B,H]: (32, 4)\n",
      "meta keys: ['tile_id', 'start_time', 'lat', 'lon']\n",
      "tile_id[0]: 30.5_25.0\n",
      "start_time[0]: 2021-01-18 06:00:00\n",
      "lat/lon tensors: (32,) (32,)\n",
      "\n",
      "=== PATCHIFY SMOKE TEST ===\n",
      "P=4, S=2 -> N=3\n",
      "tokens shape [B,N,P*F]: (32, 3, 200)\n",
      "\n",
      "All sanity checks passed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def collate_keep_meta(batch):\n",
    "    # tensors\n",
    "    x = torch.stack([b[\"x_hist\"] for b in batch])      # [B, L, F]\n",
    "    y = torch.stack([b[\"y_future\"] for b in batch])    # [B, H]\n",
    "    lat = torch.tensor([b[\"lat\"] for b in batch], dtype=torch.float32)\n",
    "    lon = torch.tensor([b[\"lon\"] for b in batch], dtype=torch.float32)\n",
    "    # keep metadata as simple Python lists/strings\n",
    "    meta = {\n",
    "        \"tile_id\":   [b[\"tile_id\"] for b in batch],\n",
    "        # stringify Timestamps\n",
    "        \"start_time\": [str(b[\"start_time\"]) for b in batch],\n",
    "        \"lat\": lat, \"lon\": lon,\n",
    "    }\n",
    "    return {\"x_hist\": x, \"y_future\": y, \"meta\": meta}\n",
    "\n",
    "\n",
    "# ---------- 1) Identify columns ----------\n",
    "NON_FEATURE_COLS = {\n",
    "    \"lon\", \"lat\", \"time\", \"source_file\", \"PM25_MERRA2\", \"PM25_ug_m3\", \"class\"\n",
    "}\n",
    "\n",
    "\n",
    "def get_feature_cols(df: pd.DataFrame):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    feat_cols = [c for c in num_cols if c not in NON_FEATURE_COLS and c.lower() not in {\n",
    "        \"timestamp\"}]\n",
    "    return feat_cols\n",
    "\n",
    "# ---------- 2) Parse & tidy ----------\n",
    "\n",
    "\n",
    "def prepare_dataframe(df: pd.DataFrame, hourly=True, dayfirst=True, freq=\"H\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Parses timestamps from df['time'].\n",
    "    - Builds tile_id from (lat, lon).\n",
    "    - Optionally resamples per tile to an evenly spaced time grid (freq='H' or '30T').\n",
    "    - Returns a tidy numeric frame where features are numeric and PM25_ug_m3 is present.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) timestamp + tile id\n",
    "    df[\"timestamp\"] = pd.to_datetime(\n",
    "        df[\"time\"], dayfirst=dayfirst, errors=\"coerce\")\n",
    "    df[\"tile_id\"] = (df[\"lat\"].round(4).astype(str) +\n",
    "                     \"_\" + df[\"lon\"].round(4).astype(str))\n",
    "\n",
    "    keep = [\"timestamp\", \"tile_id\", \"lat\", \"lon\",\n",
    "            \"PM25_ug_m3\"] + get_feature_cols(df)\n",
    "    df = df[keep].dropna(subset=[\"timestamp\"]).sort_values(\n",
    "        [\"tile_id\", \"timestamp\"])\n",
    "\n",
    "    if hourly:\n",
    "        def _resample(g):\n",
    "            tile = g[\"tile_id\"].iloc[0]\n",
    "            lat0 = float(g[\"lat\"].iloc[0])\n",
    "            lon0 = float(g[\"lon\"].iloc[0])\n",
    "\n",
    "            g = g.set_index(\"timestamp\").sort_index()\n",
    "\n",
    "            # numeric-only columns for resampling (avoid strings like tile_id)\n",
    "            num_cols = g.select_dtypes(include=[np.number]).columns\n",
    "            # numeric_only implicitly True on numeric subset\n",
    "            g_num = g[num_cols].resample(freq).mean()\n",
    "\n",
    "            # fill gaps\n",
    "            g_num = g_num.interpolate(\"time\").ffill().bfill()\n",
    "\n",
    "            # add metadata back\n",
    "            g_num[\"tile_id\"] = tile\n",
    "            g_num[\"lat\"] = lat0\n",
    "            g_num[\"lon\"] = lon0\n",
    "\n",
    "            return g_num.reset_index()\n",
    "\n",
    "        df = df.groupby(\"tile_id\", group_keys=False).apply(_resample)\n",
    "\n",
    "    # ensure numeric dtypes and fill any leftovers\n",
    "    feat_cols = get_feature_cols(df)\n",
    "    df[feat_cols + [\"PM25_ug_m3\"]] = df[feat_cols +\n",
    "                                        [\"PM25_ug_m3\"]].astype(float).fillna(0.0)\n",
    "    return df\n",
    "\n",
    "# ---------- 3) Compute normalization stats ----------\n",
    "\n",
    "\n",
    "def compute_feature_stats(df: pd.DataFrame):\n",
    "    feat_cols = get_feature_cols(df)\n",
    "    mean = df[feat_cols].mean().astype(\"float32\").values\n",
    "    std = df[feat_cols].std(ddof=0).replace(0, 1.0).astype(\"float32\").values\n",
    "    return feat_cols, mean, std\n",
    "\n",
    "# ---------- 4) Window index builder ----------\n",
    "\n",
    "\n",
    "def build_indices(df: pd.DataFrame, L=168, H=72, stride=1):\n",
    "    idx = []\n",
    "    for tile, g in df.groupby(\"tile_id\"):\n",
    "        n = len(g)\n",
    "        for t in range(L, n - H + 1, stride):  # note +1\n",
    "            idx.append((tile, t))\n",
    "    return idx\n",
    "\n",
    "# ---------- 5) PyTorch Dataset ----------\n",
    "\n",
    "\n",
    "class TSWindowDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, L=168, H=72, stride=1, stats=None):\n",
    "        \"\"\"\n",
    "        df: output of prepare_dataframe()\n",
    "        L: lookback length (hours)\n",
    "        H: horizon length (72)\n",
    "        stats: (feat_cols, mean, std) from compute_feature_stats(train_df)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.L, self.H = L, H\n",
    "        self.feat_cols, self.mean, self.std = stats if stats is not None else compute_feature_stats(\n",
    "            df)\n",
    "        self.idx = build_indices(df, L, H, stride)\n",
    "\n",
    "        # pre-slice groups to avoid repeated groupby in __getitem__\n",
    "        self.groups = {tile: g.reset_index(drop=True)\n",
    "                       for tile, g in df.groupby(\"tile_id\")}\n",
    "\n",
    "    def __len__(self): return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        tile, t = self.idx[i]\n",
    "        g = self.groups[tile]\n",
    "\n",
    "        # history window [t-L .. t-1]\n",
    "        hist = g.loc[t-self.L:t-1,\n",
    "                     self.feat_cols].values.astype(np.float32)   # [L, F]\n",
    "\n",
    "        # <- add .astype(np.float32)\n",
    "        hist = ((hist - self.mean) / self.std).astype(np.float32)\n",
    "\n",
    "        # future targets [t .. t+H-1]\n",
    "        fut = g.loc[t:t+self.H-1,\n",
    "                    \"PM25_ug_m3\"].values.astype(np.float32)       # [H]\n",
    "\n",
    "        # metadata\n",
    "        start_ts = g.loc[t, \"timestamp\"]\n",
    "        lat = float(g[\"lat\"].iloc[0])\n",
    "        lon = float(g[\"lon\"].iloc[0])\n",
    "\n",
    "        return {\n",
    "            \"x_hist\": torch.from_numpy(hist),        # [L, F]\n",
    "            \"y_future\": torch.from_numpy(fut),       # [H]\n",
    "            \"tile_id\": tile,\n",
    "            \"start_time\": pd.Timestamp(start_ts),\n",
    "            \"lat\": lat, \"lon\": lon,\n",
    "        }\n",
    "\n",
    "\n",
    "# ---------- 6) Quick usage + rich debug ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load a CSV just for this run (swap to parquet reader when ready)\n",
    "    # expects: time, lat, lon, PM25_ug_m3 + numeric features\n",
    "    df_raw = pd.read_csv(\"data.csv\")\n",
    "\n",
    "    # Build tidy frame\n",
    "    df = prepare_dataframe(df_raw, hourly=True, dayfirst=True, freq=\"H\")\n",
    "\n",
    "    # Config (small so you see windows immediately)\n",
    "    L, H, stride = 8, 4, 1\n",
    "\n",
    "    # Feature stats\n",
    "    feat_cols, mean, std = compute_feature_stats(df)\n",
    "    F = len(feat_cols)\n",
    "\n",
    "    # Dataset\n",
    "    ds = TSWindowDataset(df, L=L, H=H, stride=stride,\n",
    "                         stats=(feat_cols, mean, std))\n",
    "\n",
    "    # ----------------- High-level summary -----------------\n",
    "    print(\"\\n=== DATA SUMMARY ===\")\n",
    "    print(\"df shape:\", df.shape)\n",
    "    print(\"time range:\", df[\"timestamp\"].min(), \"→\", df[\"timestamp\"].max())\n",
    "    print(\"#tiles:\", df[\"tile_id\"].nunique())\n",
    "    print(\"#features (F):\", F)\n",
    "    print(\"first 10 feature cols:\", feat_cols[:10])\n",
    "    print(f\"window config: L={L}, H={H}, stride={stride}\")\n",
    "    print(\"#windows:\", len(ds))\n",
    "\n",
    "    # Per-tile coverage (top/bottom few)\n",
    "    counts = df.groupby(\"tile_id\")[\"timestamp\"].count().sort_values()\n",
    "    print(\"\\nrows per tile (smallest 5):\")\n",
    "    print(counts.head(5))\n",
    "    print(\"rows per tile (largest 5):\")\n",
    "    print(counts.tail(5))\n",
    "\n",
    "    if len(ds) == 0:\n",
    "        raise SystemExit(\n",
    "            \"\\n[!] No windows available. Increase coverage or lower L/H.\")\n",
    "\n",
    "    # ----------------- Inspect first window -----------------\n",
    "    print(\"\\n=== FIRST WINDOW DEBUG ===\")\n",
    "    tile0, t0 = ds.idx[0]\n",
    "    g0 = ds.groups[tile0]\n",
    "    b0 = ds[0]\n",
    "    x0, y0 = b0[\"x_hist\"], b0[\"y_future\"]\n",
    "\n",
    "    print(\"tile_id:\", tile0)\n",
    "    print(\"history shape [L,F]:\", tuple(x0.shape),\n",
    "          \"| future shape [H]:\", tuple(y0.shape))\n",
    "\n",
    "    # time ranges for history & future\n",
    "    hist_ts = g0.loc[t0-L:t0-1, \"timestamp\"].to_list()\n",
    "    fut_ts = g0.loc[t0:t0+H-1, \"timestamp\"].to_list()\n",
    "    print(\"hist timestamps:\", hist_ts[0], \"→\", hist_ts[-1])\n",
    "    print(\"fut  timestamps:\", fut_ts[0],  \"→\", fut_ts[-1])\n",
    "\n",
    "    # check spacing is hourly\n",
    "    hist_deltas = pd.Series(hist_ts).diff(\n",
    "    ).dropna().dt.total_seconds().unique()\n",
    "    fut_deltas = pd.Series(fut_ts).diff().dropna().dt.total_seconds().unique()\n",
    "    print(\"hist Δt seconds (unique):\", hist_deltas)\n",
    "    print(\"fut  Δt seconds (unique):\", fut_deltas)\n",
    "\n",
    "    # normalization sanity: mean ~ 0, std ~ 1 over this window (rough check)\n",
    "    print(\"x_hist window mean/std (overall):\",\n",
    "          float(x0.mean()), float(x0.std()))\n",
    "    # show a single feature’s first few timesteps\n",
    "    print(\"x_hist[0:5, 0] sample:\", x0[:5, 0].tolist())\n",
    "    # show a few future targets\n",
    "    print(\"y_future sample:\", y0[:min(5, H)].tolist())\n",
    "\n",
    "    # ----------------- Batch check via DataLoader -----------------\n",
    "    loader = DataLoader(ds, batch_size=32, shuffle=True, drop_last=True,\n",
    "                        collate_fn=collate_keep_meta)\n",
    "\n",
    "    batch = next(iter(loader))\n",
    "    Xb, Yb = batch[\"x_hist\"], batch[\"y_future\"]\n",
    "    meta = batch[\"meta\"]\n",
    "\n",
    "    print(\"\\n=== BATCH CHECK (custom collate) ===\")\n",
    "    print(\"batch x_hist shape [B,L,F]:\", tuple(Xb.shape))\n",
    "    print(\"batch y_future shape [B,H]:\", tuple(Yb.shape))\n",
    "    print(\"meta keys:\", list(meta.keys()))\n",
    "    print(\"tile_id[0]:\", meta[\"tile_id\"][0])\n",
    "    print(\"start_time[0]:\", meta[\"start_time\"][0])\n",
    "    print(\"lat/lon tensors:\",\n",
    "          tuple(meta[\"lat\"].shape), tuple(meta[\"lon\"].shape))\n",
    "\n",
    "    # ----------------- PatchTST token check (dev aid) -----------------\n",
    "    # If you plan to patchify later, this shows expected token count.\n",
    "    def patchify(x, P=16, S=8):  # x: [B,L,F] -> [B,N,P*F]\n",
    "        B, L_, F_ = x.shape\n",
    "        N = (L_ - P) // S + 1\n",
    "        return torch.stack([x[:, s:s+P, :].reshape(B, P*F_) for s in range(0, L_-P+1, S)], dim=1)\n",
    "\n",
    "    P, S = 4, 2  # small numbers just to visualize with L=8\n",
    "    tokens = patchify(Xb, P=P, S=S)\n",
    "    print(\"\\n=== PATCHIFY SMOKE TEST ===\")\n",
    "    print(f\"P={P}, S={S} -> N={(L-P)//S + 1}\")\n",
    "    print(\"tokens shape [B,N,P*F]:\", tuple(tokens.shape))\n",
    "\n",
    "    print(\"\\nAll sanity checks passed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878bd114-975f-4116-8587-3af50b0d8d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- time-based split (80/20 by timestamp) ----\n",
    "cutoff = df[\"timestamp\"].quantile(0.80)\n",
    "train_df = df[df[\"timestamp\"] <= cutoff].copy()\n",
    "val_df = df[df[\"timestamp\"] > cutoff].copy()\n",
    "\n",
    "# ---- stats on train only ----\n",
    "feat_cols, mean, std = compute_feature_stats(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293db734-eee5-4726-a993-8cb5584d95bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLS feature dim: 288\n"
     ]
    }
   ],
   "source": [
    "# 1) Gather row-wise feature matrices (no windowing here)\n",
    "Xtr_rows = train_df[feat_cols].values\n",
    "Xva_rows = val_df[feat_cols].values\n",
    "\n",
    "# 2) Configure BLS as a preprocessor\n",
    "cfg = BLSConfig(\n",
    "    n_feature_groups=16,\n",
    "    feature_group_size=12,\n",
    "    n_enhancement_groups=8,\n",
    "    enhancement_group_size=12,\n",
    "    feature_activation=\"tanh\",\n",
    "    enhancement_activation=\"tanh\",\n",
    "    lambda_reg=1e-2,\n",
    "    add_bias=False,\n",
    "    standardize=True,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "# 3) Fit BLS on training rows (dummy y to stay on regression path)\n",
    "dummy_y = np.zeros((len(Xtr_rows), 2), dtype=float)\n",
    "bls = BroadLearningSystem(cfg).fit(Xtr_rows, dummy_y)\n",
    "\n",
    "# 4) Transform rows -> BLS features (vectorized, no loops)\n",
    "# [n_train_rows, D_bls]\n",
    "Ztr_rows = bls.extract_features(Xtr_rows)\n",
    "# [n_val_rows,   D_bls]\n",
    "Zva_rows = bls.extract_features(Xva_rows)\n",
    "D_bls = Ztr_rows.shape[1]\n",
    "bls_cols = [f\"BLS_{i}\" for i in range(D_bls)]\n",
    "print(\"BLS feature dim:\", D_bls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97668ce-05a5-4e1f-b928-47c42271fdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#BLS features (F): 288\n"
     ]
    }
   ],
   "source": [
    "# 5) Build new frames: keep metadata/target + add all BLS cols at once\n",
    "keep_cols = [\"timestamp\", \"tile_id\", \"lat\", \"lon\", \"PM25_ug_m3\"]\n",
    "Ztr_df = pd.DataFrame(Ztr_rows, columns=bls_cols,\n",
    "                      index=train_df.index).astype(\"float32\")\n",
    "Zva_df = pd.DataFrame(Zva_rows, columns=bls_cols,\n",
    "                      index=val_df.index).astype(\"float32\")\n",
    "train_df_bls = pd.concat([train_df[keep_cols], Ztr_df],\n",
    "                         axis=1).reset_index(drop=True)\n",
    "val_df_bls = pd.concat([val_df[keep_cols],   Zva_df],\n",
    "                       axis=1).reset_index(drop=True)\n",
    "\n",
    "# 6) From here on, treat BLS features as the only inputs\n",
    "#    (NON_FEATURE_COLS already excludes PM25_ug_m3, lat/lon, etc.)\n",
    "feat_cols_bls, mean_bls, std_bls = compute_feature_stats(train_df_bls)\n",
    "F = len(feat_cols_bls)\n",
    "print(\"#BLS features (F):\", F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a2e89-c703-4aaf-a141-bba1df34dea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train windows: 5620 | #val windows: 100\n"
     ]
    }
   ],
   "source": [
    "# 7) Rebuild datasets/loaders on the BLS-augmented frames\n",
    "from torch.utils.data import DataLoader\n",
    "L, H, stride = 168, 72, 1\n",
    "train_ds = TSWindowDataset(train_df_bls, L=L, H=H, stride=stride, stats=(\n",
    "    feat_cols_bls, mean_bls, std_bls))\n",
    "val_ds = TSWindowDataset(val_df_bls,   L=L, H=H, stride=stride, stats=(\n",
    "    feat_cols_bls, mean_bls, std_bls))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,\n",
    "                          drop_last=True,  collate_fn=collate_keep_meta)\n",
    "val_loader = DataLoader(val_ds,   batch_size=32, shuffle=False,\n",
    "                        drop_last=False, collate_fn=collate_keep_meta)\n",
    "\n",
    "print(\"#train windows:\", len(train_ds), \"| #val windows:\", len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03256ec7-98f3-4788-ac28-ecf55f032780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "def sinusoidal_positional_encoding(n_pos: int, d_model: int, device=None):\n",
    "    pe = torch.zeros(n_pos, d_model, device=device)\n",
    "    pos = torch.arange(0, n_pos, device=device).unsqueeze(1).float()\n",
    "    div = torch.exp(torch.arange(\n",
    "        0, d_model, 2, device=device).float() * (-math.log(10000.0)/d_model))\n",
    "    pe[:, 0::2] = torch.sin(pos * div)\n",
    "    pe[:, 1::2] = torch.cos(pos * div)\n",
    "    return pe  # [N, d]\n",
    "\n",
    "\n",
    "class PatchPosEncoder(nn.Module):\n",
    "    def __init__(self, in_features, patch_len=16, stride=8, d_model=128):\n",
    "        super().__init__()\n",
    "        self.P, self.S, self.F, self.d = patch_len, stride, in_features, d_model\n",
    "        self.proj = nn.Linear(self.P * self.F, self.d)\n",
    "\n",
    "    def forward(self, x):           # x: [B, L, F]\n",
    "        B, L, F = x.shape\n",
    "        starts = range(0, L - self.P + 1, self.S)\n",
    "        patches = [\n",
    "            x[:, s:s+self.P, :].reshape(B, self.P*self.F) for s in starts]\n",
    "        T = torch.stack(patches, dim=1)              # [B, N, P*F]\n",
    "        T = self.proj(T)                             # [B, N, d]\n",
    "        pe = sinusoidal_positional_encoding(T.size(1), self.d, device=x.device)\n",
    "        return T + pe                                # [B, N, d]\n",
    "\n",
    "\n",
    "class SimplePatcherHead(nn.Module):\n",
    "    def __init__(self, in_features, L, H, patch_len=16, stride=8, d_model=128):\n",
    "        super().__init__()\n",
    "        self.enc = PatchPosEncoder(in_features, patch_len, stride, d_model)\n",
    "        self.head = nn.Linear(d_model, H)\n",
    "\n",
    "    def forward(self, x_hist):      # [B, L, F]\n",
    "        tokens = self.enc(x_hist)   # [B, N, d]\n",
    "        pooled = tokens[:, -1]      # last-token pool (or tokens.mean(dim=1))\n",
    "\n",
    "        return self.head(pooled)    # [B, H]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a17b8-9505-40d3-a2b7-f2a0732191f3",
   "metadata": {},
   "source": [
    "# LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf17d76-469a-4d36-b21f-5153e34a962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Classical LSTM cell + wrapper that uses your PatchPosEncoder ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ClassicalLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Classical LSTM cell implementation:\n",
    "      Uses standard PyTorch LSTM but with manual cell implementation for consistency.\n",
    "      Input -> LSTM gates -> standard LSTM updates\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Use PyTorch's built-in LSTM for efficiency\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        x: [B, N, input_size]  (your Patch+PosEnc tokens)\n",
    "        returns: hidden_seq [B, N, hidden], (h_T, c_T)\n",
    "        \"\"\"\n",
    "        B, N, _ = x.size()\n",
    "\n",
    "        if init_states is None:\n",
    "            h_0 = torch.zeros(self.num_layers, B,\n",
    "                              self.hidden_size, device=x.device, dtype=x.dtype)\n",
    "            c_0 = torch.zeros(self.num_layers, B,\n",
    "                              self.hidden_size, device=x.device, dtype=x.dtype)\n",
    "            init_states = (h_0, c_0)\n",
    "\n",
    "        # LSTM forward pass\n",
    "        hidden_seq, (h_T, c_T) = self.lstm(x, init_states)\n",
    "\n",
    "        return hidden_seq, (h_T, c_T)\n",
    "\n",
    "\n",
    "class PatchToLSTM72(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchPosEncoder (yours) -> Classical LSTM over tokens -> Linear -> H (e.g., 72).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, L, H, patch_len=16, stride=8,\n",
    "                 d_model=128, hidden_size=128, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        # uses your existing class\n",
    "        self.enc = PatchPosEncoder(in_features, patch_len, stride, d_model)\n",
    "        self.lstm = ClassicalLSTM(input_size=d_model, hidden_size=hidden_size,\n",
    "                                  num_layers=num_layers, dropout=dropout)\n",
    "        self.head = nn.Linear(hidden_size, H)\n",
    "\n",
    "    def forward(self, x_hist):               # x_hist: [B, L, F]\n",
    "        tokens = self.enc(x_hist)            # [B, N, d_model]\n",
    "        h_seq, (hT, cT) = self.lstm(tokens)  # [B, N, hidden]\n",
    "        return self.head(h_seq[:, -1, :])    # [B, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de255f4-f99e-44c8-be18-7a6e9f1d96da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLSTM (with BLS features) forward pass OK.\n"
     ]
    }
   ],
   "source": [
    "# --- Build the Classical LSTM model (must run before the training loop) ---\n",
    "F_bls = len(feat_cols_bls)\n",
    "\n",
    "lstm_model = PatchToLSTM72(\n",
    "    in_features=F_bls, L=L, H=H,\n",
    "    patch_len=16, stride=8,\n",
    "    d_model=128, hidden_size=128,\n",
    "    num_layers=1, dropout=0.0\n",
    ")\n",
    "\n",
    "# quick smoke test\n",
    "with torch.no_grad():\n",
    "    _ = lstm_model(next(iter(train_loader))[\"x_hist\"][:2])\n",
    "print(\"Classical LSTM (with BLS features) forward pass OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3eb82e-2439-4bed-a9ab-66c9642decee",
   "metadata": {},
   "source": [
    "# GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0991ea-c5da-47ce-ae75-ed38ef838974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Classical GRU cell + wrapper that uses your PatchPosEncoder (drop-in alongside LSTM) ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ClassicalGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    Classical GRU cell implementation:\n",
    "      Uses standard PyTorch GRU for efficiency.\n",
    "      Standard GRU gates: reset, update, and new candidate state\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Use PyTorch's built-in GRU for efficiency\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, init_state=None):\n",
    "        \"\"\"\n",
    "        x: [B, N, input_size]  tokens from Patch+PosEnc\n",
    "        returns: hidden_seq [B, N, hidden], h_T\n",
    "        \"\"\"\n",
    "        B, N, _ = x.size()\n",
    "\n",
    "        if init_state is None:\n",
    "            h_0 = torch.zeros(self.num_layers, B,\n",
    "                              self.hidden_size, device=x.device, dtype=x.dtype)\n",
    "        else:\n",
    "            h_0 = init_state.unsqueeze(\n",
    "                0) if init_state.dim() == 2 else init_state\n",
    "\n",
    "        # GRU forward pass\n",
    "        hidden_seq, h_T = self.gru(x, h_0)\n",
    "\n",
    "        return hidden_seq, h_T.squeeze(0) if h_T.size(0) == 1 else h_T\n",
    "\n",
    "\n",
    "class PatchToGRU72(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchPosEncoder (yours) -> Classical GRU over tokens -> Linear -> H (e.g., 72).\n",
    "    Same interface as PatchToLSTM72 so you can swap easily.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, L, H, patch_len=16, stride=8,\n",
    "                 d_model=128, hidden_size=128, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = PatchPosEncoder(in_features, patch_len, stride, d_model)\n",
    "        self.gru = ClassicalGRU(input_size=d_model, hidden_size=hidden_size,\n",
    "                                num_layers=num_layers, dropout=dropout)\n",
    "        self.head = nn.Linear(hidden_size, H)\n",
    "\n",
    "    def forward(self, x_hist):                # x_hist: [B, L, F]\n",
    "        tokens = self.enc(x_hist)             # [B, N, d_model]\n",
    "        h_seq, hT = self.gru(tokens)          # [B, N, hidden], [B, hidden]\n",
    "        return self.head(h_seq[:, -1, :])     # [B, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56898146-91ae-4678-86cf-4d059f60105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QGRU model built and forward pass OK.\n"
     ]
    }
   ],
   "source": [
    "# Build Classical GRU model\n",
    "F_bls = len(feat_cols_bls)\n",
    "\n",
    "gru_model = PatchToGRU72(\n",
    "    in_features=F_bls, L=L, H=H,\n",
    "    patch_len=16, stride=8,\n",
    "    d_model=128, hidden_size=128,\n",
    "    num_layers=1, dropout=0.0\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = gru_model(next(iter(train_loader))[\"x_hist\"][:2])\n",
    "print(\"Classical GRU model built and forward pass OK.\")\n",
    "\n",
    "# You can choose which model to train by uncommenting one of these lines:\n",
    "# For LSTM training:\n",
    "# model = lstm_model\n",
    "\n",
    "# For GRU training:\n",
    "model = gru_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372132d-2ade-4e42-9056-a3d38c170487",
   "metadata": {},
   "source": [
    "# Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ec7c2-5b42-43d8-a8dd-31cd5c78f179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu for 30 epochs...\n",
      "Epoch 001 | train 2670.8546 (147.2s) | val 2534.4421 (2.1s) | lr 1.00e-03\n",
      "Epoch 002 | train 2662.7704 (146.6s) | val 2534.4607 (2.1s) | lr 9.97e-04\n",
      "Epoch 003 | train 2663.9504 (146.8s) | val 2534.4114 (2.1s) | lr 9.89e-04\n"
     ]
    }
   ],
   "source": [
    "# === Train LSTM/GRU for multiple epochs and plot the loss ===\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- config ---\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "GRAD_CLIP = 1.0  # helps stabilize training\n",
    "PRINT_EVERY = 1\n",
    "\n",
    "# Use GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "# optional cosine decay\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "\n",
    "def run_epoch(model, loader, train: bool):\n",
    "    model.train(train)\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    start = time.time()\n",
    "    for batch in loader:\n",
    "        x = batch[\"x_hist\"].to(device)          # [B, L, F]\n",
    "        y = batch[\"y_future\"].to(device)        # [B, H]\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(train):\n",
    "            y_hat = model(x)                    # [B, H]\n",
    "            loss = criterion(y_hat, y)\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                if GRAD_CLIP is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        model.parameters(), GRAD_CLIP)\n",
    "                optimizer.step()\n",
    "        total += float(loss) * x.size(0)\n",
    "        count += x.size(0)\n",
    "    elapsed = time.time() - start\n",
    "    return total / max(count, 1), elapsed\n",
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "lrs = []\n",
    "\n",
    "print(f\"Training on {device} for {EPOCHS} epochs...\")\n",
    "best_val = math.inf\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_time = run_epoch(model, train_loader, train=True)\n",
    "    va_loss, va_time = run_epoch(model, val_loader,   train=False)\n",
    "    train_losses.append(tr_loss)\n",
    "    val_losses.append(va_loss)\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        best_state = {k: v.cpu().clone()\n",
    "                      for k, v in model.state_dict().items()}\n",
    "\n",
    "    if epoch % PRINT_EVERY == 0:\n",
    "        print(f\"Epoch {epoch:03d} | \"\n",
    "              f\"train {tr_loss:.4f} ({tr_time:.1f}s) | \"\n",
    "              f\"val {va_loss:.4f} ({va_time:.1f}s) | \"\n",
    "              f\"lr {lrs[-1]:.2e}\")\n",
    "\n",
    "# (optional) load best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"\\nLoaded best model (val MSE = {best_val:.4f}).\")\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(7, 4.5))\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.title(\"Classical LSTM/GRU: training and validation loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (optional) plot learning rate\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(lrs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"learning rate\")\n",
    "plt.title(\"Learning rate schedule\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad4334-e7b4-4267-8b83-436571922498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model Evaluation and Comparison ===\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"Evaluate model performance on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[\"x_hist\"].to(device)\n",
    "            y = batch[\"y_future\"].to(device)\n",
    "            y_hat = model(x)\n",
    "\n",
    "            predictions.append(y_hat.cpu().numpy())\n",
    "            targets.append(y.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(targets, predictions)\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'predictions': predictions,\n",
    "        'targets': targets\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate the trained model\n",
    "print(\"=== Final Model Evaluation ===\")\n",
    "train_metrics = evaluate_model(model, train_loader, device)\n",
    "val_metrics = evaluate_model(model, val_loader, device)\n",
    "\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  MSE: {train_metrics['mse']:.4f}\")\n",
    "print(f\"  MAE: {train_metrics['mae']:.4f}\")\n",
    "print(f\"  RMSE: {train_metrics['rmse']:.4f}\")\n",
    "print(f\"  R²: {train_metrics['r2']:.4f}\")\n",
    "\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  MSE: {val_metrics['mse']:.4f}\")\n",
    "print(f\"  MAE: {val_metrics['mae']:.4f}\")\n",
    "print(f\"  RMSE: {val_metrics['rmse']:.4f}\")\n",
    "print(f\"  R²: {val_metrics['r2']:.4f}\")\n",
    "\n",
    "# Plot predictions vs targets for validation set\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(val_metrics['targets'][:1000, 0],\n",
    "            val_metrics['predictions'][:1000, 0], alpha=0.5)\n",
    "plt.plot([val_metrics['targets'][:, 0].min(), val_metrics['targets'][:, 0].max()],\n",
    "         [val_metrics['targets'][:, 0].min(), val_metrics['targets'][:, 0].max()], 'r--')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Predictions vs True Values (Hour 1)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = val_metrics['predictions'][:1000, 0] - \\\n",
    "    val_metrics['targets'][:1000, 0]\n",
    "plt.scatter(val_metrics['predictions'][:1000, 0], residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot (Hour 1)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Model Training Complete ===\")\n",
    "print(\"The pipeline successfully:\")\n",
    "print(\"1. Preprocessed data with BLS feature extraction\")\n",
    "print(\"2. Created time series windows\")\n",
    "print(\"3. Applied patch-based positional encoding\")\n",
    "print(\"4. Trained classical LSTM/GRU for PM2.5 forecasting\")\n",
    "print(\"5. Evaluated model performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenva)",
   "language": "python",
   "name": "myenva"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
