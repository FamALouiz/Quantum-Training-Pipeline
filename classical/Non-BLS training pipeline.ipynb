{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386b79d-4ad1-4bbc-8698-f4ef555ac255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA SUMMARY ===\n",
      "df shape: (24400, 55)\n",
      "time range: 2021-01-10 00:00:00 → 2021-02-10 19:00:00\n",
      "#tiles: 400\n",
      "#features (F): 50\n",
      "first 10 feature cols: ['DUEXTTAU', 'BCFLUXU', 'OCFLUXV', 'BCANGSTR', 'SUFLUXV', 'SSSMASS25', 'SSSMASS', 'OCSMASS', 'BCCMASS', 'BCSMASS']\n",
      "window config: L=8, H=4, stride=1\n",
      "#windows: 20000\n",
      "\n",
      "rows per tile (smallest 5):\n",
      "tile_id\n",
      "26.5_36.875    24\n",
      "28.5_30.625    24\n",
      "28.5_30.0      24\n",
      "28.5_29.375    24\n",
      "28.5_28.75     24\n",
      "Name: timestamp, dtype: int64\n",
      "rows per tile (largest 5):\n",
      "tile_id\n",
      "31.0_25.0    764\n",
      "26.0_25.0    764\n",
      "27.5_25.0    764\n",
      "25.5_25.0    764\n",
      "22.0_25.0    764\n",
      "Name: timestamp, dtype: int64\n",
      "\n",
      "=== FIRST WINDOW DEBUG ===\n",
      "tile_id: 22.0_25.0\n",
      "history shape [L,F]: (8, 50) | future shape [H]: (4,)\n",
      "hist timestamps: 2021-01-10 00:00:00 → 2021-01-10 07:00:00\n",
      "fut  timestamps: 2021-01-10 08:00:00 → 2021-01-10 11:00:00\n",
      "hist Δt seconds (unique): [3600.]\n",
      "fut  Δt seconds (unique): [3600.]\n",
      "x_hist window mean/std (overall): -0.3407861292362213 0.7213065028190613\n",
      "x_hist[0:5, 0] sample: [0.03592585772275925, 0.04441426694393158, 0.05713042989373207, 0.0698879063129425, 0.09415498375892639]\n",
      "y_future sample: [26.396438598632812, 25.872926712036133, 26.150850296020508, 27.159780502319336]\n",
      "\n",
      "=== BATCH CHECK (custom collate) ===\n",
      "batch x_hist shape [B,L,F]: (32, 8, 50)\n",
      "batch y_future shape [B,H]: (32, 4)\n",
      "meta keys: ['tile_id', 'start_time', 'lat', 'lon']\n",
      "tile_id[0]: 27.5_25.0\n",
      "start_time[0]: 2021-01-17 07:00:00\n",
      "lat/lon tensors: (32,) (32,)\n",
      "\n",
      "=== PATCHIFY SMOKE TEST ===\n",
      "P=4, S=2 -> N=3\n",
      "tokens shape [B,N,P*F]: (32, 3, 200)\n",
      "\n",
      "All sanity checks passed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def collate_keep_meta(batch):\n",
    "    # tensors\n",
    "    x = torch.stack([b[\"x_hist\"] for b in batch])      # [B, L, F]\n",
    "    y = torch.stack([b[\"y_future\"] for b in batch])    # [B, H]\n",
    "    lat = torch.tensor([b[\"lat\"] for b in batch], dtype=torch.float32)\n",
    "    lon = torch.tensor([b[\"lon\"] for b in batch], dtype=torch.float32)\n",
    "    # keep metadata as simple Python lists/strings\n",
    "    meta = {\n",
    "        \"tile_id\":   [b[\"tile_id\"] for b in batch],\n",
    "        # stringify Timestamps\n",
    "        \"start_time\": [str(b[\"start_time\"]) for b in batch],\n",
    "        \"lat\": lat, \"lon\": lon,\n",
    "    }\n",
    "    return {\"x_hist\": x, \"y_future\": y, \"meta\": meta}\n",
    "\n",
    "\n",
    "# ---------- 1) Identify columns ----------\n",
    "NON_FEATURE_COLS = {\n",
    "    \"lon\", \"lat\", \"time\", \"source_file\", \"PM25_MERRA2\", \"PM25_ug_m3\", \"class\"\n",
    "}\n",
    "\n",
    "\n",
    "def get_feature_cols(df: pd.DataFrame):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    feat_cols = [c for c in num_cols if c not in NON_FEATURE_COLS and c.lower() not in {\n",
    "        \"timestamp\"}]\n",
    "    return feat_cols\n",
    "\n",
    "# ---------- 2) Parse & tidy ----------\n",
    "\n",
    "\n",
    "def prepare_dataframe(df: pd.DataFrame, hourly=True, dayfirst=True, freq=\"H\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Parses timestamps from df['time'].\n",
    "    - Builds tile_id from (lat, lon).\n",
    "    - Optionally resamples per tile to an evenly spaced time grid (freq='H' or '30T').\n",
    "    - Returns a tidy numeric frame where features are numeric and PM25_ug_m3 is present.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) timestamp + tile id\n",
    "    df[\"timestamp\"] = pd.to_datetime(\n",
    "        df[\"time\"], dayfirst=dayfirst, errors=\"coerce\")\n",
    "    df[\"tile_id\"] = (df[\"lat\"].round(4).astype(str) +\n",
    "                     \"_\" + df[\"lon\"].round(4).astype(str))\n",
    "\n",
    "    keep = [\"timestamp\", \"tile_id\", \"lat\", \"lon\",\n",
    "            \"PM25_ug_m3\"] + get_feature_cols(df)\n",
    "    df = df[keep].dropna(subset=[\"timestamp\"]).sort_values(\n",
    "        [\"tile_id\", \"timestamp\"])\n",
    "\n",
    "    if hourly:\n",
    "        def _resample(g):\n",
    "            tile = g[\"tile_id\"].iloc[0]\n",
    "            lat0 = float(g[\"lat\"].iloc[0])\n",
    "            lon0 = float(g[\"lon\"].iloc[0])\n",
    "\n",
    "            g = g.set_index(\"timestamp\").sort_index()\n",
    "\n",
    "            # numeric-only columns for resampling (avoid strings like tile_id)\n",
    "            num_cols = g.select_dtypes(include=[np.number]).columns\n",
    "            # numeric_only implicitly True on numeric subset\n",
    "            g_num = g[num_cols].resample(freq).mean()\n",
    "\n",
    "            # fill gaps\n",
    "            g_num = g_num.interpolate(\"time\").ffill().bfill()\n",
    "\n",
    "            # add metadata back\n",
    "            g_num[\"tile_id\"] = tile\n",
    "            g_num[\"lat\"] = lat0\n",
    "            g_num[\"lon\"] = lon0\n",
    "\n",
    "            return g_num.reset_index()\n",
    "\n",
    "        df = df.groupby(\"tile_id\", group_keys=False).apply(_resample)\n",
    "\n",
    "    # ensure numeric dtypes and fill any leftovers\n",
    "    feat_cols = get_feature_cols(df)\n",
    "    df[feat_cols + [\"PM25_ug_m3\"]] = df[feat_cols +\n",
    "                                        [\"PM25_ug_m3\"]].astype(float).fillna(0.0)\n",
    "    return df\n",
    "\n",
    "# ---------- 3) Compute normalization stats ----------\n",
    "\n",
    "\n",
    "def compute_feature_stats(df: pd.DataFrame):\n",
    "    feat_cols = get_feature_cols(df)\n",
    "    mean = df[feat_cols].mean().astype(\"float32\").values\n",
    "    std = df[feat_cols].std(ddof=0).replace(0, 1.0).astype(\"float32\").values\n",
    "    return feat_cols, mean, std\n",
    "\n",
    "# ---------- 4) Window index builder ----------\n",
    "\n",
    "\n",
    "def build_indices(df: pd.DataFrame, L=168, H=72, stride=1):\n",
    "    idx = []\n",
    "    for tile, g in df.groupby(\"tile_id\"):\n",
    "        n = len(g)\n",
    "        for t in range(L, n - H + 1, stride):  # note +1\n",
    "            idx.append((tile, t))\n",
    "    return idx\n",
    "\n",
    "# ---------- 5) PyTorch Dataset ----------\n",
    "\n",
    "\n",
    "class TSWindowDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, L=168, H=72, stride=1, stats=None):\n",
    "        \"\"\"\n",
    "        df: output of prepare_dataframe()\n",
    "        L: lookback length (hours)\n",
    "        H: horizon length (72)\n",
    "        stats: (feat_cols, mean, std) from compute_feature_stats(train_df)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.L, self.H = L, H\n",
    "        self.feat_cols, self.mean, self.std = stats if stats is not None else compute_feature_stats(\n",
    "            df)\n",
    "        self.idx = build_indices(df, L, H, stride)\n",
    "\n",
    "        # pre-slice groups to avoid repeated groupby in __getitem__\n",
    "        self.groups = {tile: g.reset_index(drop=True)\n",
    "                       for tile, g in df.groupby(\"tile_id\")}\n",
    "\n",
    "    def __len__(self): return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        tile, t = self.idx[i]\n",
    "        g = self.groups[tile]\n",
    "\n",
    "        # history window [t-L .. t-1]\n",
    "        hist = g.loc[t-self.L:t-1,\n",
    "                     self.feat_cols].values.astype(np.float32)   # [L, F]\n",
    "\n",
    "        # <- add .astype(np.float32)\n",
    "        hist = ((hist - self.mean) / self.std).astype(np.float32)\n",
    "\n",
    "        # future targets [t .. t+H-1]\n",
    "        fut = g.loc[t:t+self.H-1,\n",
    "                    \"PM25_ug_m3\"].values.astype(np.float32)       # [H]\n",
    "\n",
    "        # metadata\n",
    "        start_ts = g.loc[t, \"timestamp\"]\n",
    "        lat = float(g[\"lat\"].iloc[0])\n",
    "        lon = float(g[\"lon\"].iloc[0])\n",
    "\n",
    "        return {\n",
    "            \"x_hist\": torch.from_numpy(hist),        # [L, F]\n",
    "            \"y_future\": torch.from_numpy(fut),       # [H]\n",
    "            \"tile_id\": tile,\n",
    "            \"start_time\": pd.Timestamp(start_ts),\n",
    "            \"lat\": lat, \"lon\": lon,\n",
    "        }\n",
    "\n",
    "\n",
    "# ---------- 6) Quick usage + rich debug ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load a CSV just for this run (swap to parquet reader when ready)\n",
    "    # expects: time, lat, lon, PM25_ug_m3 + numeric features\n",
    "    df_raw = pd.read_csv(\"data.csv\")\n",
    "\n",
    "    # Build tidy frame\n",
    "    df = prepare_dataframe(df_raw, hourly=True, dayfirst=True, freq=\"H\")\n",
    "\n",
    "    # Config (small so you see windows immediately)\n",
    "    L, H, stride = 8, 4, 1\n",
    "\n",
    "    # Feature stats\n",
    "    feat_cols, mean, std = compute_feature_stats(df)\n",
    "    F = len(feat_cols)\n",
    "\n",
    "    # Dataset\n",
    "    ds = TSWindowDataset(df, L=L, H=H, stride=stride,\n",
    "                         stats=(feat_cols, mean, std))\n",
    "\n",
    "    # ----------------- High-level summary -----------------\n",
    "    print(\"\\n=== DATA SUMMARY ===\")\n",
    "    print(\"df shape:\", df.shape)\n",
    "    print(\"time range:\", df[\"timestamp\"].min(), \"→\", df[\"timestamp\"].max())\n",
    "    print(\"#tiles:\", df[\"tile_id\"].nunique())\n",
    "    print(\"#features (F):\", F)\n",
    "    print(\"first 10 feature cols:\", feat_cols[:10])\n",
    "    print(f\"window config: L={L}, H={H}, stride={stride}\")\n",
    "    print(\"#windows:\", len(ds))\n",
    "\n",
    "    # Per-tile coverage (top/bottom few)\n",
    "    counts = df.groupby(\"tile_id\")[\"timestamp\"].count().sort_values()\n",
    "    print(\"\\nrows per tile (smallest 5):\")\n",
    "    print(counts.head(5))\n",
    "    print(\"rows per tile (largest 5):\")\n",
    "    print(counts.tail(5))\n",
    "\n",
    "    if len(ds) == 0:\n",
    "        raise SystemExit(\n",
    "            \"\\n[!] No windows available. Increase coverage or lower L/H.\")\n",
    "\n",
    "    # ----------------- Inspect first window -----------------\n",
    "    print(\"\\n=== FIRST WINDOW DEBUG ===\")\n",
    "    tile0, t0 = ds.idx[0]\n",
    "    g0 = ds.groups[tile0]\n",
    "    b0 = ds[0]\n",
    "    x0, y0 = b0[\"x_hist\"], b0[\"y_future\"]\n",
    "\n",
    "    print(\"tile_id:\", tile0)\n",
    "    print(\"history shape [L,F]:\", tuple(x0.shape),\n",
    "          \"| future shape [H]:\", tuple(y0.shape))\n",
    "\n",
    "    # time ranges for history & future\n",
    "    hist_ts = g0.loc[t0-L:t0-1, \"timestamp\"].to_list()\n",
    "    fut_ts = g0.loc[t0:t0+H-1, \"timestamp\"].to_list()\n",
    "    print(\"hist timestamps:\", hist_ts[0], \"→\", hist_ts[-1])\n",
    "    print(\"fut  timestamps:\", fut_ts[0],  \"→\", fut_ts[-1])\n",
    "\n",
    "    # check spacing is hourly\n",
    "    hist_deltas = pd.Series(hist_ts).diff(\n",
    "    ).dropna().dt.total_seconds().unique()\n",
    "    fut_deltas = pd.Series(fut_ts).diff().dropna().dt.total_seconds().unique()\n",
    "    print(\"hist Δt seconds (unique):\", hist_deltas)\n",
    "    print(\"fut  Δt seconds (unique):\", fut_deltas)\n",
    "\n",
    "    # normalization sanity: mean ~ 0, std ~ 1 over this window (rough check)\n",
    "    print(\"x_hist window mean/std (overall):\",\n",
    "          float(x0.mean()), float(x0.std()))\n",
    "    # show a single feature’s first few timesteps\n",
    "    print(\"x_hist[0:5, 0] sample:\", x0[:5, 0].tolist())\n",
    "    # show a few future targets\n",
    "    print(\"y_future sample:\", y0[:min(5, H)].tolist())\n",
    "\n",
    "    # ----------------- Batch check via DataLoader -----------------\n",
    "    loader = DataLoader(ds, batch_size=32, shuffle=True, drop_last=True,\n",
    "                        collate_fn=collate_keep_meta)\n",
    "\n",
    "    batch = next(iter(loader))\n",
    "    Xb, Yb = batch[\"x_hist\"], batch[\"y_future\"]\n",
    "    meta = batch[\"meta\"]\n",
    "\n",
    "    print(\"\\n=== BATCH CHECK (custom collate) ===\")\n",
    "    print(\"batch x_hist shape [B,L,F]:\", tuple(Xb.shape))\n",
    "    print(\"batch y_future shape [B,H]:\", tuple(Yb.shape))\n",
    "    print(\"meta keys:\", list(meta.keys()))\n",
    "    print(\"tile_id[0]:\", meta[\"tile_id\"][0])\n",
    "    print(\"start_time[0]:\", meta[\"start_time\"][0])\n",
    "    print(\"lat/lon tensors:\",\n",
    "          tuple(meta[\"lat\"].shape), tuple(meta[\"lon\"].shape))\n",
    "\n",
    "    # ----------------- PatchTST token check (dev aid) -----------------\n",
    "    # If you plan to patchify later, this shows expected token count.\n",
    "    def patchify(x, P=16, S=8):  # x: [B,L,F] -> [B,N,P*F]\n",
    "        B, L_, F_ = x.shape\n",
    "        N = (L_ - P) // S + 1\n",
    "        return torch.stack([x[:, s:s+P, :].reshape(B, P*F_) for s in range(0, L_-P+1, S)], dim=1)\n",
    "\n",
    "    P, S = 4, 2  # small numbers just to visualize with L=8\n",
    "    tokens = patchify(Xb, P=P, S=S)\n",
    "    print(\"\\n=== PATCHIFY SMOKE TEST ===\")\n",
    "    print(f\"P={P}, S={S} -> N={(L-P)//S + 1}\")\n",
    "    print(\"tokens shape [B,N,P*F]:\", tuple(tokens.shape))\n",
    "\n",
    "    print(\"\\nAll sanity checks passed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293db734-eee5-4726-a993-8cb5584d95bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train windows: 5620 | #val windows: 100\n"
     ]
    }
   ],
   "source": [
    "# ---- time-based split (80/20 by timestamp) ----\n",
    "cutoff = df[\"timestamp\"].quantile(0.80)\n",
    "train_df = df[df[\"timestamp\"] <= cutoff].copy()\n",
    "val_df = df[df[\"timestamp\"] > cutoff].copy()\n",
    "\n",
    "# ---- stats on train only ----\n",
    "feat_cols, mean, std = compute_feature_stats(train_df)\n",
    "\n",
    "# ---- datasets ----\n",
    "L, H, stride = 168, 72, 1   # real config\n",
    "train_ds = TSWindowDataset(\n",
    "    train_df, L=L, H=H, stride=stride, stats=(feat_cols, mean, std))\n",
    "val_ds = TSWindowDataset(\n",
    "    val_df,   L=L, H=H, stride=stride, stats=(feat_cols, mean, std))\n",
    "\n",
    "# ---- loaders (keep your custom collate) ----\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,\n",
    "                          drop_last=True, collate_fn=collate_keep_meta)\n",
    "val_loader = DataLoader(val_ds,   batch_size=32, shuffle=False,\n",
    "                        drop_last=False, collate_fn=collate_keep_meta)\n",
    "\n",
    "print(\"#train windows:\", len(train_ds), \"| #val windows:\", len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03256ec7-98f3-4788-ac28-ecf55f032780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "def sinusoidal_positional_encoding(n_pos: int, d_model: int, device=None):\n",
    "    pe = torch.zeros(n_pos, d_model, device=device)\n",
    "    pos = torch.arange(0, n_pos, device=device).unsqueeze(1).float()\n",
    "    div = torch.exp(torch.arange(\n",
    "        0, d_model, 2, device=device).float() * (-math.log(10000.0)/d_model))\n",
    "    pe[:, 0::2] = torch.sin(pos * div)\n",
    "    pe[:, 1::2] = torch.cos(pos * div)\n",
    "    return pe  # [N, d]\n",
    "\n",
    "\n",
    "class PatchPosEncoder(nn.Module):\n",
    "    def __init__(self, in_features, patch_len=16, stride=8, d_model=128):\n",
    "        super().__init__()\n",
    "        self.P, self.S, self.F, self.d = patch_len, stride, in_features, d_model\n",
    "        self.proj = nn.Linear(self.P * self.F, self.d)\n",
    "\n",
    "    def forward(self, x):           # x: [B, L, F]\n",
    "        B, L, F = x.shape\n",
    "        starts = range(0, L - self.P + 1, self.S)\n",
    "        patches = [\n",
    "            x[:, s:s+self.P, :].reshape(B, self.P*self.F) for s in starts]\n",
    "        T = torch.stack(patches, dim=1)              # [B, N, P*F]\n",
    "        T = self.proj(T)                             # [B, N, d]\n",
    "        pe = sinusoidal_positional_encoding(T.size(1), self.d, device=x.device)\n",
    "        return T + pe                                # [B, N, d]\n",
    "\n",
    "\n",
    "class SimplePatcherHead(nn.Module):\n",
    "    def __init__(self, in_features, L, H, patch_len=16, stride=8, d_model=128):\n",
    "        super().__init__()\n",
    "        self.enc = PatchPosEncoder(in_features, patch_len, stride, d_model)\n",
    "        self.head = nn.Linear(d_model, H)\n",
    "\n",
    "    def forward(self, x_hist):      # [B, L, F]\n",
    "        tokens = self.enc(x_hist)   # [B, N, d]\n",
    "        pooled = tokens[:, -1]      # last-token pool (or tokens.mean(dim=1))\n",
    "\n",
    "        return self.head(pooled)    # [B, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c4a00-8d21-4aa6-a65a-4fc77032cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat: torch.Size([32, 72])\n"
     ]
    }
   ],
   "source": [
    "F = len(feat_cols)\n",
    "model = SimplePatcherHead(in_features=F, L=L, H=H,\n",
    "                          patch_len=16, stride=8, d_model=128)\n",
    "batch = next(iter(train_loader))\n",
    "x, y = batch[\"x_hist\"], batch[\"y_future\"]\n",
    "y_hat = model(x)\n",
    "print(\"y_hat:\", y_hat.shape)  # [B, 72]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a17b8-9505-40d3-a2b7-f2a0732191f3",
   "metadata": {},
   "source": [
    "# LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf17d76-469a-4d36-b21f-5153e34a962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Classical LSTM cell + wrapper that uses your PatchPosEncoder ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ClassicalLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Classical LSTM cell implementation:\n",
    "      Uses standard PyTorch LSTM but with manual cell implementation for consistency.\n",
    "      Input -> LSTM gates -> standard LSTM updates\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Use PyTorch's built-in LSTM for efficiency\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        x: [B, N, input_size]  (your Patch+PosEnc tokens)\n",
    "        returns: hidden_seq [B, N, hidden], (h_T, c_T)\n",
    "        \"\"\"\n",
    "        B, N, _ = x.size()\n",
    "\n",
    "        if init_states is None:\n",
    "            h_0 = torch.zeros(self.num_layers, B,\n",
    "                              self.hidden_size, device=x.device, dtype=x.dtype)\n",
    "            c_0 = torch.zeros(self.num_layers, B,\n",
    "                              self.hidden_size, device=x.device, dtype=x.dtype)\n",
    "            init_states = (h_0, c_0)\n",
    "\n",
    "        # LSTM forward pass\n",
    "        hidden_seq, (h_T, c_T) = self.lstm(x, init_states)\n",
    "\n",
    "        return hidden_seq, (h_T, c_T)\n",
    "\n",
    "\n",
    "class PatchToLSTM72(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchPosEncoder (yours) -> Classical LSTM over tokens -> Linear -> H (e.g., 72).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, L, H, patch_len=16, stride=8,\n",
    "                 d_model=128, hidden_size=128, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        # uses your existing class\n",
    "        self.enc = PatchPosEncoder(in_features, patch_len, stride, d_model)\n",
    "        self.lstm = ClassicalLSTM(input_size=d_model, hidden_size=hidden_size,\n",
    "                                  num_layers=num_layers, dropout=dropout)\n",
    "        self.head = nn.Linear(hidden_size, H)\n",
    "\n",
    "    def forward(self, x_hist):               # x_hist: [B, L, F]\n",
    "        tokens = self.enc(x_hist)            # [B, N, d_model]\n",
    "        h_seq, (hT, cT) = self.lstm(tokens)  # [B, N, hidden]\n",
    "        return self.head(h_seq[:, -1, :])    # [B, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de255f4-f99e-44c8-be18-7a6e9f1d96da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLSTM model built and forward pass OK.\n"
     ]
    }
   ],
   "source": [
    "# --- Build the Classical LSTM model (must run before the training loop) ---\n",
    "F = len(feat_cols)  # number of features from your dataframe pipeline\n",
    "\n",
    "lstm_model = PatchToLSTM72(\n",
    "    in_features=F, L=L, H=H,\n",
    "    patch_len=16, stride=8,\n",
    "    d_model=128, hidden_size=128,\n",
    "    num_layers=1, dropout=0.0\n",
    ")\n",
    "\n",
    "# (optional) quick smoke test so we fail early if shapes don't match\n",
    "with torch.no_grad():\n",
    "    _ = lstm_model(next(iter(train_loader))[\"x_hist\"][:2])\n",
    "print(\"Classical LSTM model built and forward pass OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3eb82e-2439-4bed-a9ab-66c9642decee",
   "metadata": {},
   "source": [
    "# GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0991ea-c5da-47ce-ae75-ed38ef838974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Classical GRU cell + wrapper that uses your PatchPosEncoder (drop-in alongside LSTM) ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ClassicalGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    Classical GRU cell implementation:\n",
    "      Uses standard PyTorch GRU for efficiency.\n",
    "      Standard GRU gates: reset, update, and new candidate state\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Use PyTorch's built-in GRU for efficiency\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, init_state=None):\n",
    "        \"\"\"\n",
    "        x: [B, N, input_size]  tokens from Patch+PosEnc\n",
    "        returns: hidden_seq [B, N, hidden], h_T\n",
    "        \"\"\"\n",
    "        B, N, _ = x.size()\n",
    "\n",
    "        if init_state is None:\n",
    "            h_0 = torch.zeros(self.num_layers, B,\n",
    "                              self.hidden_size, device=x.device, dtype=x.dtype)\n",
    "        else:\n",
    "            h_0 = init_state.unsqueeze(\n",
    "                0) if init_state.dim() == 2 else init_state\n",
    "\n",
    "        # GRU forward pass\n",
    "        hidden_seq, h_T = self.gru(x, h_0)\n",
    "\n",
    "        return hidden_seq, h_T.squeeze(0) if h_T.size(0) == 1 else h_T\n",
    "\n",
    "\n",
    "class PatchToGRU72(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchPosEncoder (yours) -> Classical GRU over tokens -> Linear -> H (e.g., 72).\n",
    "    Same interface as PatchToLSTM72 so you can swap easily.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, L, H, patch_len=16, stride=8,\n",
    "                 d_model=128, hidden_size=128, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = PatchPosEncoder(in_features, patch_len, stride, d_model)\n",
    "        self.gru = ClassicalGRU(input_size=d_model, hidden_size=hidden_size,\n",
    "                                num_layers=num_layers, dropout=dropout)\n",
    "        self.head = nn.Linear(hidden_size, H)\n",
    "\n",
    "    def forward(self, x_hist):                # x_hist: [B, L, F]\n",
    "        tokens = self.enc(x_hist)             # [B, N, d_model]\n",
    "        h_seq, hT = self.gru(tokens)          # [B, N, hidden], [B, hidden]\n",
    "        return self.head(h_seq[:, -1, :])     # [B, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56898146-91ae-4678-86cf-4d059f60105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QGRU model built and forward pass OK.\n"
     ]
    }
   ],
   "source": [
    "# Build Classical GRU model\n",
    "F = len(feat_cols)  # number of features from your dataframe pipeline\n",
    "\n",
    "gru_model = PatchToGRU72(\n",
    "    in_features=F, L=L, H=H,\n",
    "    patch_len=16, stride=8,\n",
    "    d_model=128, hidden_size=128,\n",
    "    num_layers=1, dropout=0.0\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = gru_model(next(iter(train_loader))[\"x_hist\"][:2])\n",
    "print(\"Classical GRU model built and forward pass OK.\")\n",
    "\n",
    "# You can choose which model to train by uncommenting one of these lines:\n",
    "# For LSTM training:\n",
    "# model = lstm_model\n",
    "\n",
    "# For GRU training:\n",
    "model = gru_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372132d-2ade-4e42-9056-a3d38c170487",
   "metadata": {},
   "source": [
    "# Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ec7c2-5b42-43d8-a8dd-31cd5c78f179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu for 30 epochs...\n",
      "Epoch 001 | train 2386.8321 (143.7s) | val 1973.1548 (2.0s) | lr 1.00e-03\n",
      "Epoch 002 | train 1754.0602 (143.0s) | val 1386.1457 (2.1s) | lr 9.97e-04\n",
      "Epoch 003 | train 1154.2798 (144.2s) | val 951.5430 (2.1s) | lr 9.89e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Train LSTM/GRU for multiple epochs and plot the loss ===\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- config ---\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "GRAD_CLIP = 1.0  # helps stabilize training\n",
    "PRINT_EVERY = 1\n",
    "\n",
    "# Use GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "# optional cosine decay\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "\n",
    "def run_epoch(model, loader, train: bool):\n",
    "    model.train(train)\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    start = time.time()\n",
    "    for batch in loader:\n",
    "        x = batch[\"x_hist\"].to(device)          # [B, L, F]\n",
    "        y = batch[\"y_future\"].to(device)        # [B, H]\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(train):\n",
    "            y_hat = model(x)                    # [B, H]\n",
    "            loss = criterion(y_hat, y)\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                if GRAD_CLIP is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        model.parameters(), GRAD_CLIP)\n",
    "                optimizer.step()\n",
    "        total += float(loss) * x.size(0)\n",
    "        count += x.size(0)\n",
    "    elapsed = time.time() - start\n",
    "    return total / max(count, 1), elapsed\n",
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "lrs = []\n",
    "\n",
    "print(f\"Training on {device} for {EPOCHS} epochs...\")\n",
    "best_val = math.inf\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_time = run_epoch(model, train_loader, train=True)\n",
    "    va_loss, va_time = run_epoch(model, val_loader,   train=False)\n",
    "    train_losses.append(tr_loss)\n",
    "    val_losses.append(va_loss)\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        best_state = {k: v.cpu().clone()\n",
    "                      for k, v in model.state_dict().items()}\n",
    "\n",
    "    if epoch % PRINT_EVERY == 0:\n",
    "        print(f\"Epoch {epoch:03d} | \"\n",
    "              f\"train {tr_loss:.4f} ({tr_time:.1f}s) | \"\n",
    "              f\"val {va_loss:.4f} ({va_time:.1f}s) | \"\n",
    "              f\"lr {lrs[-1]:.2e}\")\n",
    "\n",
    "# (optional) load best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"\\nLoaded best model (val MSE = {best_val:.4f}).\")\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(7, 4.5))\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.title(\"Classical LSTM/GRU: training and validation loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (optional) plot learning rate\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(lrs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"learning rate\")\n",
    "plt.title(\"Learning rate schedule\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad4334-e7b4-4267-8b83-436571922498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model Evaluation and Comparison ===\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"Evaluate model performance on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[\"x_hist\"].to(device)\n",
    "            y = batch[\"y_future\"].to(device)\n",
    "            y_hat = model(x)\n",
    "\n",
    "            predictions.append(y_hat.cpu().numpy())\n",
    "            targets.append(y.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(targets, predictions)\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'predictions': predictions,\n",
    "        'targets': targets\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate the trained model\n",
    "print(\"=== Final Model Evaluation ===\")\n",
    "train_metrics = evaluate_model(model, train_loader, device)\n",
    "val_metrics = evaluate_model(model, val_loader, device)\n",
    "\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  MSE: {train_metrics['mse']:.4f}\")\n",
    "print(f\"  MAE: {train_metrics['mae']:.4f}\")\n",
    "print(f\"  RMSE: {train_metrics['rmse']:.4f}\")\n",
    "print(f\"  R²: {train_metrics['r2']:.4f}\")\n",
    "\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  MSE: {val_metrics['mse']:.4f}\")\n",
    "print(f\"  MAE: {val_metrics['mae']:.4f}\")\n",
    "print(f\"  RMSE: {val_metrics['rmse']:.4f}\")\n",
    "print(f\"  R²: {val_metrics['r2']:.4f}\")\n",
    "\n",
    "# Plot predictions vs targets for validation set\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(val_metrics['targets'][:1000, 0],\n",
    "            val_metrics['predictions'][:1000, 0], alpha=0.5)\n",
    "plt.plot([val_metrics['targets'][:, 0].min(), val_metrics['targets'][:, 0].max()],\n",
    "         [val_metrics['targets'][:, 0].min(), val_metrics['targets'][:, 0].max()], 'r--')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Predictions vs True Values (Hour 1)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = val_metrics['predictions'][:1000, 0] - \\\n",
    "    val_metrics['targets'][:1000, 0]\n",
    "plt.scatter(val_metrics['predictions'][:1000, 0], residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot (Hour 1)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Model Training Complete ===\")\n",
    "print(\"The pipeline successfully:\")\n",
    "print(\"1. Preprocessed raw data with time series windowing\")\n",
    "print(\"2. Applied patch-based positional encoding\")\n",
    "print(\"3. Trained classical LSTM/GRU for PM2.5 forecasting\")\n",
    "print(\"4. Evaluated model performance\")\n",
    "print(\"Note: This pipeline uses raw features without BLS preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc0064e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pm25_to_aqi(pm25_value: float):\n",
    "    \"\"\"\n",
    "    Convert PM2.5 concentration (µg/m³) to AQI using US EPA breakpoints.\n",
    "    Returns a tuple (AQI_value, category).\n",
    "    \"\"\"\n",
    "    # Breakpoints: (PM_low, PM_high, AQI_low, AQI_high, Category)\n",
    "    breakpoints = [\n",
    "        (0.0, 12.0, 0, 50, \"Good\"),\n",
    "        (12.1, 35.4, 51, 100, \"Moderate\"),\n",
    "        (35.5, 55.4, 101, 150, \"Unhealthy for Sensitive Groups\"),\n",
    "        (55.5, 150.4, 151, 200, \"Unhealthy\"),\n",
    "        (150.5, 250.4, 201, 300, \"Very Unhealthy\"),\n",
    "        (250.5, 350.4, 301, 400, \"Hazardous\"),\n",
    "        (350.5, 500.4, 401, 500, \"Hazardous\"),\n",
    "    ]\n",
    "\n",
    "    for BP_lo, BP_hi, I_lo, I_hi, category in breakpoints:\n",
    "        if BP_lo <= pm25_value <= BP_hi:\n",
    "            aqi = (I_hi - I_lo) / (BP_hi - BP_lo) * (pm25_value - BP_lo) + I_lo\n",
    "            return round(aqi), category\n",
    "\n",
    "    # Out of range\n",
    "    return None, \"Value out of range (0-500.4 µg/m³)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
